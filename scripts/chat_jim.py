
# ðŸ—‚ï¸ Folder Structure:
# .
# â”œâ”€â”€ data/
# â”‚   â””â”€â”€ prepare_outmd.py           # Convert Markdown to instruction-tuning format
# â”œâ”€â”€ scripts/
# â”‚   â”œâ”€â”€ mlx_train.py               # Fine-tuning with mlx-lm (Phi-3 on Apple Silicon)
# â”‚   â”œâ”€â”€ chat.py                # Query your fine-tuned Phi-3 model
# â”‚   â”œâ”€â”€ train_rm.py                # Reward model training (optional)
# â”‚   â””â”€â”€ run_dpo.py                 # Direct Preference Optimization (optional)
# â”œâ”€â”€ serve/
# â”‚   â””â”€â”€ serve_ollama.py            # Serve model via Ollama or llama.cpp
# â””â”€â”€ eval/
#     â””â”€â”€ eval_story_mimic.py        # Embedding-based story match scoring

# -----------------------------
# scripts/chat.py
# -----------------------------
from mlx_lm import load, generate
from mlx_lm.utils import load_model
import readline

from pathlib import Path
MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"
WEIGHTS_PATH = Path( "phi3-mlx/model.safetensors" )

PROMPT_TEMPLATE = """
You are St. John's Jim, a myth-weaving, bar-stool Buddha of the Pacific Northwest.
Tell a new short story in your usual voice. Base it on this seed:
"""

model, tokenizer = load(MODEL_NAME)

print("\nðŸŒ€ Chatting with the Jim-tuned model. Type your story seed. Ctrl+C to exit.\n")

while True:
    try:
        user_input = input("Seed > ").strip()
        if not user_input:
            continue

        full_prompt = PROMPT_TEMPLATE + user_input + "\n"
        response = generate(model, tokenizer, prompt=full_prompt, verbose=False, max_tokens=512)
        print("\nðŸ“˜ Jim says:\n" + response + "\n")

    except (KeyboardInterrupt, EOFError):
        print("\nðŸ‘‹ Goodbye!")
        break

# -----------------------------
# Usage:
# $ python scripts/chat.py
# Type a story seed, get a response in Jim's voice.
# -----------------------------
