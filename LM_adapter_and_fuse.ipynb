{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf9e514-7ad7-4e72-b5bb-3981bbe5b57c",
   "metadata": {},
   "source": [
    "## fine-tuning with LM\n",
    "Step 1.\tRun Manifest & Environment\n",
    "\tâ€¢\tInputs: desired Python version; tool choice (uv/poetry/pip); seed\n",
    "\tâ€¢\tOutputs: run_manifest.yaml (python version, platform, chip, mlx-lm version, commit hashes), locked deps (requirements.lock or poetry.lock), RNG seeds set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b944d636-45cd-4e86-9150-68644d1c1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 â€” Run Manifest & Environment (Apple Silicon / MLX)\n",
    "# - Captures exact runtime info (OS, chip, Python, key libs)\n",
    "# - Locks dependencies via `pip freeze` -> requirements.lock\n",
    "# - Sets deterministic seeds (random, numpy; PYTHONHASHSEED)\n",
    "# - Writes manifest to run_manifest.yaml (falls back to JSON if PyYAML missing)\n",
    "\n",
    "import os, sys, platform, subprocess, json, time, hashlib, shlex\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Configuration (edit as needed) ----------\n",
    "from config_loader import load_config\n",
    "CFG = load_config()  # pulls default.yaml, then local.yaml, then CFG_* env, then (optionally) pass a dict of overrides\n",
    "#print(\"JIM\",CFG)\n",
    "OUT_DIR = Path(CFG.run.output_dir)                  # where to write outputs\n",
    "LOCKFILE = OUT_DIR / \"requirements.lock\"\n",
    "MANIFEST_YAML = OUT_DIR / \"run_manifest.yaml\"\n",
    "MANIFEST_JSON = OUT_DIR / \"run_manifest.json\"\n",
    "SEED = CFG.run.seed\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# 1) Set seeds for determinism (Python & NumPy)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    import numpy as np\n",
    "    np.random.seed(SEED)\n",
    "    numpy_ver = np.__version__\n",
    "except Exception:\n",
    "    numpy_ver = None\n",
    "\n",
    "# 2) Collect environment info\n",
    "def _safe_import_version(pkg_name):\n",
    "    try:\n",
    "        import importlib.metadata as md\n",
    "        return md.version(pkg_name)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _which(cmd):\n",
    "    try:\n",
    "        r = subprocess.run([\"which\", cmd], capture_output=True, text=True)\n",
    "        return r.stdout.strip() or None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        r = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "        return (r.returncode, r.stdout.strip(), r.stderr.strip())\n",
    "    except Exception as e:\n",
    "        return (1, \"\", str(e))\n",
    "\n",
    "py_ver = sys.version.split()[0]\n",
    "platform_info = {\n",
    "    \"system\": platform.system(),\n",
    "    \"release\": platform.release(),\n",
    "    \"version\": platform.version(),\n",
    "    \"machine\": platform.machine(),\n",
    "    \"processor\": platform.processor(),\n",
    "    \"python\": py_ver,\n",
    "}\n",
    "\n",
    "# Apple chip details (best-effort)\n",
    "chip_brand = None\n",
    "if platform.system() == \"Darwin\":\n",
    "    code, out, _ = _run(\"sysctl -n machdep.cpu.brand_string\")\n",
    "    chip_brand = out if code == 0 else None\n",
    "    platform_info[\"mac_ver\"] = platform.mac_ver()[0]\n",
    "platform_info[\"chip_brand\"] = chip_brand\n",
    "\n",
    "# 3) Key package versions (MLX-focused)\n",
    "mlx_lm_ver   = _safe_import_version(\"mlx-lm\")\n",
    "datasets_ver = _safe_import_version(\"datasets\")\n",
    "pandas_ver   = _safe_import_version(\"pandas\")\n",
    "tqdm_ver     = _safe_import_version(\"tqdm\")\n",
    "\n",
    "# 4) Lock dependencies with pip freeze\n",
    "LOCKFILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "code, out, err = _run(f\"{shlex.quote(sys.executable)} -m pip freeze\")\n",
    "if code == 0:\n",
    "    LOCKFILE.write_text(out + \"\\n\", encoding=\"utf-8\")\n",
    "else:\n",
    "    print(\"[warn] pip freeze failed:\", err)\n",
    "\n",
    "# Hash the lock for quick integrity checks\n",
    "lock_hash = None\n",
    "if LOCKFILE.exists():\n",
    "    lock_hash = hashlib.sha256(LOCKFILE.read_bytes()).hexdigest()\n",
    "\n",
    "# 5) Build manifest object\n",
    "manifest = {\n",
    "    \"timestamp_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "    \"seed\": SEED,\n",
    "    \"platform\": platform_info,\n",
    "    \"packages\": {\n",
    "        \"mlx-lm\": mlx_lm_ver,\n",
    "        \"datasets\": datasets_ver,\n",
    "        \"pandas\": pandas_ver,\n",
    "        \"tqdm\": tqdm_ver,\n",
    "        \"numpy\": numpy_ver,\n",
    "    },\n",
    "    \"executables\": {\n",
    "        \"python\": sys.executable,\n",
    "        \"python_which\": _which(\"python\"),\n",
    "        \"pip_which\": _which(\"pip\"),\n",
    "    },\n",
    "    \"artifacts\": {\n",
    "        \"requirements_lock\": str(LOCKFILE.resolve()) if LOCKFILE.exists() else None,\n",
    "        \"requirements_lock_sha256\": lock_hash,\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"This manifest anchors the run. Keep it with any training outputs.\",\n",
    "        \"If you change env/deps, regenerate this step to create a new lock.\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# 6) Write manifest to YAML (fallback to JSON if PyYAML not installed)\n",
    "def write_manifest_yaml(obj, path_yaml, path_json_fallback):\n",
    "    try:\n",
    "        import yaml  # type: ignore\n",
    "        with open(path_yaml, \"w\", encoding=\"utf-8\") as f:\n",
    "            yaml.safe_dump(obj, f, sort_keys=False)\n",
    "        return str(path_yaml)\n",
    "    except Exception as e:\n",
    "        # Fallback JSON\n",
    "        with open(path_json_fallback, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(obj, f, indent=2)\n",
    "        return f\"{path_yaml} (PyYAML missing) -> wrote JSON: {path_json_fallback}\"\n",
    "\n",
    "out_path = write_manifest_yaml(manifest, MANIFEST_YAML, MANIFEST_JSON)\n",
    "\n",
    "# 7) Print a compact summary\n",
    "print(\"\\n=== RUN MANIFEST SUMMARY ===\")\n",
    "print(f\"Python:        {py_ver}\")\n",
    "print(f\"OS/Chip:       {platform_info['system']} {platform_info.get('mac_ver') or platform_info['release']} | {platform_info.get('chip_brand') or platform_info['machine']}\")\n",
    "print(f\"mlx-lm:        {mlx_lm_ver}\")\n",
    "print(f\"datasets:      {datasets_ver}\")\n",
    "print(f\"pandas:        {pandas_ver}\")\n",
    "print(f\"tqdm:          {tqdm_ver}\")\n",
    "print(f\"numpy:         {numpy_ver}\")\n",
    "print(f\"Seed:          {SEED}\")\n",
    "print(f\"Lockfile:      {LOCKFILE}  sha256={lock_hash[:12]+'â€¦' if lock_hash else None}\")\n",
    "print(f\"Manifest path: {out_path}\")\n",
    "print(\"============================\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "642d2340-088b-49de-b58d-0a3597010085",
   "metadata": {},
   "source": [
    "Step 2.\tData Contract & Catalog\n",
    "\tâ€¢\tInputs: path to data dir; filename policy map (e.g., {train: \"train.jsonl\", valid: \"valid.jsonl\"}); schema = one JSON object per line with \"text\"\n",
    "\tâ€¢\tOutputs: data_contract.json (filenames, schema), data_catalog.json (sizes, byte counts, md5/sha256 of each file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c575b09-ae11-4338-9ab1-a0949cbe471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2â€² â€” HF Dataset Import (parameterized) + write data_contract.json & data_catalog.json\n",
    "\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "\n",
    "print(\"Dataset:\", cfg.data.hf_dataset)\n",
    "print(\"Subset:\", cfg.data.subset)\n",
    "print(\"Mode:\", cfg.data.mode)\n",
    "print(\"Valid fraction:\", cfg.data.valid_fraction)\n",
    "print(\"Seed:\", cfg.run.seed)\n",
    "\n",
    "# Use them in preprocessing\n",
    "HF_DATASET  = cfg.data.hf_dataset\n",
    "SUBSET      = cfg.data.subset\n",
    "MODE        = cfg.data.mode\n",
    "VALID_FRACT = cfg.data.valid_fraction\n",
    "MIN_WORDS   = cfg.data.min_words\n",
    "MAX_WORDS   = cfg.data.max_words\n",
    "SEED        = cfg.run.seed\n",
    "\n",
    "out_dir = Path(\"data\"); out_dir.mkdir(exist_ok=True)\n",
    "CONTRACT    = out_dir / cfg.paths.contract\n",
    "CATALOG     = out_dir / cfg.paths.catalog\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import json, random, hashlib, time\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "print(f\"Loading {HF_DATASET} subset={SUBSET} â€¦\")\n",
    "ds = load_dataset(HF_DATASET, name=SUBSET, split=\"train\")\n",
    "print(ds)\n",
    "\n",
    "def wc(s): return len(str(s).split())\n",
    "def sha(s): return hashlib.sha256(str(s).encode(\"utf-8\",\"ignore\")).hexdigest()\n",
    "\n",
    "rows = []\n",
    "for r in ds:\n",
    "    quote  = (r.get(\"quote\") or \"\").strip()\n",
    "    author = (r.get(\"author\") or \"\").strip()\n",
    "    if not quote:\n",
    "        continue\n",
    "\n",
    "    if MODE == \"plain\":\n",
    "        text = quote\n",
    "    else:\n",
    "        instr = f\"Write a short motivational quote in the style of {author}.\" if author else \"Write a short motivational quote.\"\n",
    "        text  = f\"Instruction:\\n{instr}\\n\\nResponse:\\n{quote}\"\n",
    "\n",
    "    if not (MIN_WORDS <= wc(text) <= MAX_WORDS):\n",
    "        continue\n",
    "    rows.append(text)\n",
    "\n",
    "# dedupe while preserving order\n",
    "seen=set(); uniq=[]\n",
    "for t in rows:\n",
    "    h=sha(t)\n",
    "    if h not in seen:\n",
    "        seen.add(h); uniq.append(t)\n",
    "\n",
    "# split\n",
    "random.shuffle(uniq)\n",
    "valid_n = max(100, int(len(uniq) * VALID_FRACT))\n",
    "valid = uniq[:valid_n]\n",
    "train = uniq[valid_n:]\n",
    "\n",
    "def write_jsonl(path: Path, texts):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for t in texts:\n",
    "            f.write(json.dumps({\"text\": t}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "train_path = out_dir / \"train.jsonl\"\n",
    "valid_path = out_dir / \"valid.jsonl\"\n",
    "write_jsonl(train_path, train)\n",
    "write_jsonl(valid_path, valid)\n",
    "\n",
    "print(f\"Wrote {len(train)} train, {len(valid)} valid to {out_dir.resolve()}\")\n",
    "\n",
    "# --- Write data_contract.json and data_catalog.json ---\n",
    "def count_lines_bytes(p: Path):\n",
    "    n = 0\n",
    "    with p.open(\"rb\") as f:\n",
    "        for _ in f: n += 1\n",
    "    return n, p.stat().st_size\n",
    "\n",
    "def sha256_file(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "created = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "\n",
    "# Contract (simple schema with detected string field = \"text\")\n",
    "data_contract = {\n",
    "    \"created_utc\": created,\n",
    "    \"data_dir\": str(out_dir.resolve()),\n",
    "    \"filenames\": {\n",
    "        \"train\": {\"chosen\": train_path.name, \"resolved\": str(train_path.resolve())},\n",
    "        \"valid\": {\"chosen\": valid_path.name, \"resolved\": str(valid_path.resolve())},\n",
    "    },\n",
    "    \"schema\": {\"format\": \"jsonl\", \"fields\": {\"text\": \"string\"}},\n",
    "}\n",
    "CONTRACT.write_text(json.dumps(data_contract, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Catalog (write BOTH legacy 'entries' and simple 'files' views)\n",
    "t_lines, t_bytes = count_lines_bytes(train_path)\n",
    "v_lines, v_bytes = count_lines_bytes(valid_path)\n",
    "t_sha = sha256_file(train_path)\n",
    "v_sha = sha256_file(valid_path)\n",
    "\n",
    "data_catalog = {\n",
    "    \"created_utc\": created,\n",
    "    \"files\": {\n",
    "        \"train\": {\"path\": str(train_path.resolve()), \"lines\": t_lines, \"bytes\": t_bytes, \"sha256\": t_sha},\n",
    "        \"valid\": {\"path\": str(valid_path.resolve()), \"lines\": v_lines, \"bytes\": v_bytes, \"sha256\": v_sha},\n",
    "    },\n",
    "    \"entries\": {\n",
    "        \"train\": {\"path\": str(train_path.resolve()), \"stats\": {\n",
    "            \"num_valid_examples\": t_lines, \"num_bytes\": t_bytes, \"sha256\": t_sha}},\n",
    "        \"valid\": {\"path\": str(valid_path.resolve()), \"stats\": {\n",
    "            \"num_valid_examples\": v_lines, \"num_bytes\": v_bytes, \"sha256\": v_sha}},\n",
    "    },\n",
    "}\n",
    "CATALOG.write_text(json.dumps(data_catalog, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote data_contract.json and data_catalog.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e45e6-6820-4b35-ab60-74663f81e069",
   "metadata": {},
   "source": [
    "Step 3.\tData Validation & Stats\n",
    "\tâ€¢\tInputs: train/valid JSONL, contract\n",
    "\tâ€¢\tOutputs: data_report.json (line counts, empty/dup checks, length histograms, charset, special-token frequencies), a few sampled rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4f69e-b7dd-4ec2-b4c5-24b34437cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 â€” Data Validation & Stats\n",
    "# Inputs:\n",
    "#   - data_contract.json (from Step 2)\n",
    "# Outputs:\n",
    "#   - data_report.json   (per-split detailed stats & issues)\n",
    "# Console:\n",
    "#   - compact summary (counts, dupes, whitespace/control issues, length percentiles)\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "from __future__ import annotations\n",
    "import json, re, unicodedata, statistics, hashlib\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "out_dir = Path(\"data\"); out_dir.mkdir(exist_ok=True)\n",
    "CONTRACT    = out_dir / cfg.paths.contract\n",
    "REPORT      = out_dir / cfg.paths.report\n",
    "\n",
    "# Heuristics: potential stop/EOS markers to scan for\n",
    "EOS_MARKERS = [\n",
    "    \"</s>\",         # common HF eos\n",
    "    \"###\",          # section break in some templates\n",
    "    \"\\n\\n\",         # blank-line stop\n",
    "    \"<|eot_id|>\",   # chat-style separators\n",
    "    \"<|endoftext|>\" # GPT-like\n",
    "]\n",
    "\n",
    "def load_contract(path: Path) -> Tuple[str, Dict[str, str], str]:\n",
    "    c = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    data_dir = c[\"data_dir\"]\n",
    "    # discover the text field (first string-type field in schema)\n",
    "    fields = c.get(\"schema\", {}).get(\"fields\", {})\n",
    "    text_field = None\n",
    "    for k, v in fields.items():\n",
    "        if str(v).lower() == \"string\":\n",
    "            text_field = k; break\n",
    "    if not text_field:\n",
    "        text_field = \"text\"  # fallback\n",
    "    files = {split: info[\"resolved\"] for split, info in c[\"filenames\"].items() if info.get(\"resolved\")}\n",
    "    return text_field, files, data_dir\n",
    "\n",
    "def hash_text(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\", \"ignore\")).hexdigest()\n",
    "\n",
    "def char_classes(s: str) -> Dict[str, int]:\n",
    "    # Count basic unicode categories and control chars\n",
    "    ctrl = sum(1 for ch in s if unicodedata.category(ch) in (\"Cc\",\"Cf\"))\n",
    "    ws   = sum(1 for ch in s if ch.isspace())\n",
    "    nonascii = sum(1 for ch in s if ord(ch) > 127)\n",
    "    return {\"control\": ctrl, \"whitespace\": ws, \"non_ascii\": nonascii}\n",
    "\n",
    "def percentiles(values: List[int], q=(5, 25, 50, 75, 95)) -> Dict[str, int]:\n",
    "    if not values: return {f\"p{p}\": 0 for p in q}\n",
    "    vals = sorted(values)\n",
    "    out = {}\n",
    "    for p in q:\n",
    "        k = max(0, min(len(vals)-1, int(round((p/100)* (len(vals)-1)))))\n",
    "        out[f\"p{p}\"] = int(vals[k])\n",
    "    return out\n",
    "\n",
    "def scan_file(path: Path, field: str) -> Dict[str, Any]:\n",
    "    n_lines = 0\n",
    "    bad_json = 0\n",
    "    missing_field = 0\n",
    "    non_str = 0\n",
    "    empty = 0\n",
    "    whitespace_only = 0\n",
    "    leading_ws = 0\n",
    "    trailing_ws = 0\n",
    "    ctrl_lines = 0\n",
    "\n",
    "    lengths = []\n",
    "    hashes = []\n",
    "    eos_hits = {m: 0 for m in EOS_MARKERS}\n",
    "\n",
    "    samples_good: List[str] = []\n",
    "    samples_bad: List[str]  = []\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for line in f:\n",
    "            n_lines += 1\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                bad_json += 1\n",
    "                if len(samples_bad) < 3: samples_bad.append(f\"[bad_json] {line[:160]}\")\n",
    "                continue\n",
    "\n",
    "            if field not in obj:\n",
    "                missing_field += 1\n",
    "                if len(samples_bad) < 3: samples_bad.append(f\"[missing_field] {line[:160]}\")\n",
    "                continue\n",
    "            val = obj[field]\n",
    "            if not isinstance(val, str):\n",
    "                non_str += 1\n",
    "                if len(samples_bad) < 3: samples_bad.append(f\"[non_string] {str(val)[:160]}\")\n",
    "                continue\n",
    "\n",
    "            if val == \"\":\n",
    "                empty += 1\n",
    "            if val.strip() == \"\":\n",
    "                whitespace_only += 1\n",
    "            if val and val[0].isspace():\n",
    "                leading_ws += 1\n",
    "            if val and val[-1].isspace():\n",
    "                trailing_ws += 1\n",
    "\n",
    "            cc = char_classes(val)\n",
    "            if cc[\"control\"] > 0:\n",
    "                ctrl_lines += 1\n",
    "\n",
    "            L = len(val)\n",
    "            lengths.append(L)\n",
    "            hashes.append(hash_text(val))\n",
    "            for m in EOS_MARKERS:\n",
    "                if m in val:\n",
    "                    eos_hits[m] += 1\n",
    "\n",
    "            if len(samples_good) < 3:\n",
    "                samples_good.append(val)\n",
    "\n",
    "    # duplicates\n",
    "    dup_count = 0\n",
    "    dup_examples = []\n",
    "    from collections import Counter\n",
    "    c = Counter(hashes)\n",
    "    for h, cnt in c.items():\n",
    "        if cnt > 1:\n",
    "            dup_count += cnt - 1\n",
    "            if len(dup_examples) < 3:\n",
    "                dup_examples.append(h)\n",
    "\n",
    "    # length stats\n",
    "    length_stats = {\n",
    "        \"count\": len(lengths),\n",
    "        \"min\": int(min(lengths)) if lengths else 0,\n",
    "        \"max\": int(max(lengths)) if lengths else 0,\n",
    "        \"mean\": float(statistics.mean(lengths)) if lengths else 0.0,\n",
    "        \"median\": float(statistics.median(lengths)) if lengths else 0.0,\n",
    "        \"percentiles\": percentiles(lengths),\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"path\": str(path),\n",
    "        \"lines\": n_lines,\n",
    "        \"valid_examples\": len(lengths),\n",
    "        \"errors\": {\n",
    "            \"bad_json\": bad_json,\n",
    "            \"missing_field\": missing_field,\n",
    "            \"non_string_field\": non_str,\n",
    "        },\n",
    "        \"empties\": {\n",
    "            \"empty_exact\": empty,\n",
    "            \"whitespace_only\": whitespace_only,\n",
    "            \"leading_whitespace\": leading_ws,\n",
    "            \"trailing_whitespace\": trailing_ws,\n",
    "        },\n",
    "        \"control_char_lines\": ctrl_lines,\n",
    "        \"duplicates\": {\n",
    "            \"duplicate_example_count\": dup_count,\n",
    "            \"sha256_examples\": dup_examples,\n",
    "        },\n",
    "        \"length_chars\": length_stats,\n",
    "        \"eos_markers_hits\": eos_hits,\n",
    "        \"samples\": {\n",
    "            \"good_first3\": samples_good,\n",
    "            \"bad_first3\": samples_bad,\n",
    "        },\n",
    "    }\n",
    "\n",
    "# Load contract and validate\n",
    "text_field, files, data_dir = load_contract(CONTRACT)\n",
    "report: Dict[str, Any] = {\n",
    "    \"created_utc\": __import__(\"time\").strftime(\"%Y-%m-%dT%H:%M:%SZ\", __import__(\"time\").gmtime()),\n",
    "    \"data_dir\": data_dir,\n",
    "    \"text_field\": text_field,\n",
    "    \"splits\": {},\n",
    "}\n",
    "\n",
    "for split, p in files.items():\n",
    "    rep = scan_file(Path(p), text_field)\n",
    "    report[\"splits\"][split] = rep\n",
    "\n",
    "# Write full report\n",
    "REPORT.write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Console summary\n",
    "print(\"=== DATA VALIDATION SUMMARY ===\")\n",
    "for split, rep in report[\"splits\"].items():\n",
    "    errs = rep[\"errors\"]\n",
    "    empt = rep[\"empties\"]\n",
    "    lens = rep[\"length_chars\"]\n",
    "    eos  = rep[\"eos_markers_hits\"]\n",
    "    dup  = rep[\"duplicates\"][\"duplicate_example_count\"]\n",
    "    print(f\"- {split}: lines={rep['lines']} valid={rep['valid_examples']} \"\n",
    "          f\"errors(bad/miss/nonstr)={errs['bad_json']}/{errs['missing_field']}/{errs['non_string_field']} \"\n",
    "          f\"empties(exact/ws/lead/trail)={empt['empty_exact']}/{empt['whitespace_only']}/{empt['leading_whitespace']}/{empt['trailing_whitespace']} \"\n",
    "          f\"dupes={dup} len[min/med/95/max]={lens['min']}/{int(lens['median'])}/{lens['percentiles']['p95']}/{lens['max']} \"\n",
    "          f\"eos_hits={{\" + \", \".join(f'{k}:{v}' for k,v in eos.items() if v) + \"}}\")\n",
    "print(\"Wrote:\", REPORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b70cf26-0629-4257-8dda-94a6514b6868",
   "metadata": {},
   "source": [
    "Step 4.\tFormatting Policy (Prompt Template)\n",
    "\tâ€¢\tInputs: selected template name (e.g., â€œplain_i/oâ€, â€œllama3_styleâ€); tokenization preview\n",
    "\tâ€¢\tOutputs: deterministic formatter function spec; example before/after table; stored as format_policy.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73183f36-fe8a-4be1-ab0b-d22059effb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 â€” Formatting Policy (Prompt Template)\n",
    "# Goal:\n",
    "#   - Pick a formatting policy for how prompts/responses should look during training & generation.\n",
    "#   - DOES NOT MODIFY your existing JSONL files.\n",
    "#   - Writes `format_policy.json` describing the chosen template + parameters.\n",
    "#\n",
    "# Inputs:\n",
    "#   - data_contract.json  (from Step 2)\n",
    "# Outputs:\n",
    "#   - format_policy.json  (template choice & settings)\n",
    "# Console:\n",
    "#   - Before/After preview for a few examples\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, textwrap\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "\n",
    "out_dir = Path(\"data\"); out_dir.mkdir(exist_ok=True)\n",
    "CONTRACT    = out_dir / cfg.paths.contract\n",
    "POLICY      = out_dir / cfg.paths.policy\n",
    "\n",
    "# ------------------------\n",
    "# 1) Choose a template\n",
    "# ------------------------\n",
    "# Options:\n",
    "#   \"plain_text_passthrough\" : use the JSONL \"text\" as-is (good when your data already contains instruction/response)\n",
    "#   \"icl_minimal\"            : a simple Q/A style wrapper (no special chat tokens)\n",
    "#   \"llama3_style\"           : a friendly chat-like wrapper (ASCII tags only)\n",
    "TEMPLATE_NAME = \"plain_text_passthrough\"\n",
    "\n",
    "# Optional stop strings you intend to use during generation probes later.\n",
    "# (These are just recorded here; not enforced yet.)\n",
    "STOP_STRINGS = [\"\\n\\n\"]   # common â€œblank lineâ€ stop\n",
    "USE_EOS_TOKEN = True      # whether to set eos_token_id in â€œdefaultâ€ runs later\n",
    "\n",
    "# ------------------------\n",
    "# 2) Load contract & sample a few rows for preview\n",
    "# ------------------------\n",
    "def load_contract(path: Path):\n",
    "    c = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    data_dir = Path(c[\"data_dir\"])\n",
    "    files = {k: v[\"resolved\"] for k, v in c[\"filenames\"].items() if v.get(\"resolved\")}\n",
    "    # detect the text field name (from schema) with fallback\n",
    "    fields = c.get(\"schema\", {}).get(\"fields\", {})\n",
    "    text_field = next((k for k,v in fields.items() if str(v).lower()==\"string\"), \"text\")\n",
    "    return data_dir, files, text_field\n",
    "\n",
    "data_dir, files, TEXT_FIELD = load_contract(CONTRACT)\n",
    "train_path = Path(files[\"train\"])\n",
    "\n",
    "def read_first_n_texts(p: Path, n: int = 3, field: str = \"text\") -> List[str]:\n",
    "    out = []\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if len(out) >= n: break\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            val = obj.get(field)\n",
    "            if isinstance(val, str):\n",
    "                out.append(val)\n",
    "    return out\n",
    "\n",
    "SAMPLES = read_first_n_texts(train_path, n=3, field=TEXT_FIELD)\n",
    "\n",
    "# ------------------------\n",
    "# 3) Define template functions (no mutation)\n",
    "# ------------------------\n",
    "def fmt_plain(text: str) -> str:\n",
    "    # return exactly as stored\n",
    "    return text\n",
    "\n",
    "def fmt_icl_minimal(text: str) -> str:\n",
    "    # Wrap the existing content as a single instructionâ†’response block.\n",
    "    # If your JSONL already contains both, this is nearly a no-op with a header.\n",
    "    return (\n",
    "        \"### Instruction\\n\"\n",
    "        \"Share an important thought.\\n\\n\"\n",
    "        \"### Response\\n\" + text.strip()\n",
    "    )\n",
    "\n",
    "def fmt_llama3_style(text: str) -> str:\n",
    "    # A neutral chat-ish style using plain ASCII delimiters.\n",
    "    # (We avoid special tokens here; real chat templates can be added later if desired.)\n",
    "    return (\n",
    "        \"<s>[INSTRUCTION]\\n\"\n",
    "        \"Share an .\\n\"\n",
    "        \"[/INSTRUCTION]\\n\"\n",
    "        \"[RESPONSE]\\n\" + text.strip() + \"\\n[/RESPONSE]</s>\"\n",
    "    )\n",
    "\n",
    "FORMATTERS = {\n",
    "    \"plain_text_passthrough\": fmt_plain,\n",
    "    \"icl_minimal\": fmt_icl_minimal,\n",
    "    \"llama3_style\": fmt_llama3_style,\n",
    "}\n",
    "\n",
    "if TEMPLATE_NAME not in FORMATTERS:\n",
    "    raise SystemExit(f\"Unknown TEMPLATE_NAME: {TEMPLATE_NAME}\")\n",
    "\n",
    "formatter = FORMATTERS[TEMPLATE_NAME]\n",
    "\n",
    "# ------------------------\n",
    "# 4) Preview: before/after for a few rows\n",
    "# ------------------------\n",
    "print(\"=== FORMAT PREVIEW ===\")\n",
    "print(f\"Template: {TEMPLATE_NAME}\")\n",
    "for i, txt in enumerate(SAMPLES, 1):\n",
    "    print(f\"\\n--- Example {i}: BEFORE ---\")\n",
    "    print(textwrap.shorten(txt.replace(\"\\n\",\" \\\\n \"), width=220, placeholder=\"â€¦\"))\n",
    "    print(\"--- Example {i}: AFTER  ---\")\n",
    "    print(textwrap.shorten(formatter(txt).replace(\"\\n\",\" \\\\n \"), width=220, placeholder=\"â€¦\"))\n",
    "\n",
    "# ------------------------\n",
    "# 5) Persist policy (for downstream steps)\n",
    "# ------------------------\n",
    "policy: Dict[str, Any] = {\n",
    "    \"template_name\": TEMPLATE_NAME,\n",
    "    \"text_field\": TEXT_FIELD,\n",
    "    \"stop_strings\": STOP_STRINGS,\n",
    "    \"use_eos_token\": USE_EOS_TOKEN,\n",
    "    \"notes\": [\n",
    "        \"This policy describes how to *format* examples when generating or when materializing new data.\",\n",
    "        \"Your current JSONL will not be changed by this step.\",\n",
    "        \"Downstream steps can choose to apply this formatter or keep passthrough depending on the experiment.\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Keep a tiny deterministic sample of BEFORE/AFTER in the policy for traceability\n",
    "policy[\"preview\"] = [\n",
    "    {\"before\": SAMPLES[i], \"after\": formatter(SAMPLES[i])} for i in range(min(2, len(SAMPLES)))\n",
    "]\n",
    "\n",
    "POLICY.write_text(json.dumps(policy, indent=2), encoding=\"utf-8\")\n",
    "print(f\"\\nWrote {POLICY}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07027e12-57ee-4ef1-99e4-fb7baa4b8cc7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e3f46e0-5ac3-4be7-8fe4-1a734d3fa7c3",
   "metadata": {},
   "source": [
    "Step 5.\tTrain/Valid Materialization (optional)\n",
    "\tâ€¢\tInputs: raw aphorism text or other source (skip if already JSONL)\n",
    "\tâ€¢\tOutputs: train.jsonl, valid.jsonl that conform to the contract; lineage recorded in data_lineage.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bca15e-b864-4a4f-b8b7-cfe7225e2ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 â€” Train/Valid Materialization (optional)\n",
    "# Behavior:\n",
    "#   - If JSONL files already exist as per data_contract.json => NO-OP (prints summary).\n",
    "#   - Else: build them from RAW_INPUT_PATH (one example per line), apply Step-4 format policy,\n",
    "#           shuffle/split, and write train/valid according to the contract's chosen filenames.\n",
    "#\n",
    "# Inputs:\n",
    "#   - data_contract.json   (from Step 2)\n",
    "#   - format_policy.json   (from Step 4)\n",
    "#   - RAW_INPUT_PATH       (used ONLY if JSONLs missing)\n",
    "# Outputs:\n",
    "#   - train.jsonl + valid/val.jsonl (only when materialization is needed)\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, random, unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "\n",
    "out_dir = Path(\"data\"); out_dir.mkdir(exist_ok=True)\n",
    "CONTRACT    = out_dir / cfg.paths.contract\n",
    "POLICY      = out_dir / cfg.paths.policy\n",
    "\n",
    "# --------- Config (edit if you need to materialize) ---------\n",
    "RAW_INPUT_PATH = Path(\"./input.txt\")  # used ONLY if JSONL missing\n",
    "MIN_LINE_WORDS = cfg.data.min_words                       # filter out very short lines\n",
    "VAL_RATIO      = 0.10                    # 10% validation split\n",
    "SHUFFLE_SEED   = cfg.run.seed\n",
    "OVERWRITE      = False                   # set True to force rebuild even if files exist\n",
    "NORMALIZE_NFC  = True                    # Unicode normalize to NFC\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def load_contract_and_policy(contract_p: Path, policy_p: Path):\n",
    "    contract = json.loads(contract_p.read_text(encoding=\"utf-8\"))\n",
    "    policy   = json.loads(policy_p.read_text(encoding=\"utf-8\"))\n",
    "    data_dir = Path(contract[\"data_dir\"])\n",
    "    files = {k: v[\"resolved\"] for k, v in contract[\"filenames\"].items() if v.get(\"resolved\")}\n",
    "    chosen = {k: v[\"chosen\"] for k, v in contract[\"filenames\"].items() if v.get(\"chosen\")}\n",
    "    text_field = next((k for k,v in contract.get(\"schema\",{}).get(\"fields\",{}).items() if str(v).lower()==\"string\"), \"text\")\n",
    "    return data_dir, files, chosen, text_field, policy\n",
    "\n",
    "def exists_and_nonempty(p: Path) -> bool:\n",
    "    return p.exists() and p.stat().st_size > 0\n",
    "\n",
    "def read_lines_raw(p: Path, min_words: int) -> List[str]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"RAW_INPUT_PATH not found: {p}\")\n",
    "    lines: List[str] = []\n",
    "    with p.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if not ln: continue\n",
    "            if len(ln.split()) < min_words: continue\n",
    "            if NORMALIZE_NFC:\n",
    "                ln = unicodedata.normalize(\"NFC\", ln)\n",
    "            lines.append(ln)\n",
    "    if not lines:\n",
    "        raise SystemExit(f\"No usable lines found in {p}.\")\n",
    "    return lines\n",
    "\n",
    "# simple formatters mirroring Step 4 names (weâ€™ll only use the selected one)\n",
    "def fmt_plain(text: str) -> str:\n",
    "    return text\n",
    "\n",
    "def fmt_icl_minimal(text: str) -> str:\n",
    "    return (\n",
    "        \"### Instruction\\n\"\n",
    "        \"Share an important thought.\\n\\n\"\n",
    "        \"### Response\\n\" + text.strip()\n",
    "    )\n",
    "\n",
    "def fmt_llama3_style(text: str) -> str:\n",
    "    return (\n",
    "        \"<s>[INSTRUCTION]\\n\"\n",
    "        \"Share an important thought.\\n\"\n",
    "        \"[/INSTRUCTION]\\n\"\n",
    "        \"[RESPONSE]\\n\" + text.strip() + \"\\n[/RESPONSE]</s>\"\n",
    "    )\n",
    "\n",
    "FORMATTERS = {\n",
    "    \"plain_text_passthrough\": fmt_plain,\n",
    "    \"icl_minimal\": fmt_icl_minimal,\n",
    "    \"llama3_style\": fmt_llama3_style,\n",
    "}\n",
    "\n",
    "def write_jsonl_text(items: List[str], out_path: Path, field: str = \"text\"):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for t in items:\n",
    "            f.write(json.dumps({field: t}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# ---- Orchestration ----\n",
    "data_dir, files_resolved, chosen_names, TEXT_FIELD, policy = load_contract_and_policy(CONTRACT, POLICY)\n",
    "train_target = Path(files_resolved.get(\"train\")) if \"train\" in files_resolved else data_dir / chosen_names.get(\"train\",\"train.jsonl\")\n",
    "valid_key = \"valid\" if \"valid\" in files_resolved or \"valid\" in chosen_names else (\"val\" if \"val\" in files_resolved or \"val\" in chosen_names else \"valid\")\n",
    "valid_target = Path(files_resolved.get(valid_key)) if valid_key in files_resolved else data_dir / chosen_names.get(valid_key, f\"{valid_key}.jsonl\")\n",
    "\n",
    "already_present = exists_and_nonempty(train_target) and exists_and_nonempty(valid_target)\n",
    "\n",
    "if already_present and not OVERWRITE:\n",
    "    print(\"âœ… JSONL already present. No materialization performed.\")\n",
    "    print(\" - train:\", train_target)\n",
    "    print(f\" - {valid_key}:\", valid_target)\n",
    "else:\n",
    "    tmpl = policy.get(\"template_name\", \"plain_text_passthrough\")\n",
    "    if tmpl not in FORMATTERS:\n",
    "        raise SystemExit(f\"Unknown template_name in format_policy.json: {tmpl}\")\n",
    "    formatter = FORMATTERS[tmpl]\n",
    "\n",
    "    raw_lines = read_lines_raw(RAW_INPUT_PATH, MIN_LINE_WORDS)\n",
    "    # apply formatter\n",
    "    examples = [formatter(x) for x in raw_lines]\n",
    "\n",
    "    # shuffle & split\n",
    "    random.seed(SHUFFLE_SEED)\n",
    "    random.shuffle(examples)\n",
    "    n = len(examples)\n",
    "    n_val = max(1, int(round(VAL_RATIO * n)))\n",
    "    val_items = examples[:n_val]\n",
    "    train_items = examples[n_val:]\n",
    "\n",
    "    # write\n",
    "    write_jsonl_text(train_items, train_target, TEXT_FIELD)\n",
    "    write_jsonl_text(val_items,   valid_target, TEXT_FIELD)\n",
    "\n",
    "    print(\"ðŸ“ Materialized JSONL files:\")\n",
    "    print(\" - train ->\", train_target, f\"(rows={len(train_items)})\")\n",
    "    print(f\" - {valid_key} ->\", valid_target, f\"(rows={len(val_items)})\")\n",
    "    print(\"Template used:\", tmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f53d8d-ed58-4dce-a75b-0d74d7b3ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5.5 (REVISED) â€” Write catalog in BOTH schemas (\"entries\" + \"files\")\n",
    "# Keeps Step 6 unchanged by emitting the legacy structure it expects.\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, hashlib, time\n",
    "from pathlib import Path\n",
    "\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "\n",
    "out_dir = Path(\"data\"); out_dir.mkdir(exist_ok=True)\n",
    "CONTRACT    = out_dir / cfg.paths.contract\n",
    "POLICY      = out_dir / cfg.paths.policy\n",
    "\n",
    "TRAIN = out_dir / \"train.jsonl\"\n",
    "VALID = out_dir / \"valid.jsonl\"\n",
    "\n",
    "def sha256_file(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def count_lines_bytes(p: Path):\n",
    "    n = 0\n",
    "    with p.open(\"rb\") as f:\n",
    "        for _ in f: n += 1\n",
    "    return n, p.stat().st_size\n",
    "\n",
    "def sniff_string_field(p: Path) -> str:\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line: continue\n",
    "            try:\n",
    "                obj=json.loads(line)\n",
    "                for k,v in obj.items():\n",
    "                    if isinstance(v,str): return k\n",
    "            except Exception:\n",
    "                pass\n",
    "    return \"text\"\n",
    "\n",
    "# sanity\n",
    "if not TRAIN.exists() or not VALID.exists():\n",
    "    raise SystemExit(\"STEP 5.5: Expected ./data/train.jsonl and ./data/valid.jsonl.\")\n",
    "\n",
    "# contract (unchanged behavior)\n",
    "created = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "string_field = sniff_string_field(TRAIN)\n",
    "contract = {\n",
    "    \"created_utc\": created,\n",
    "    \"data_dir\": str(out_dir.resolve()),\n",
    "    \"filenames\": {\n",
    "        \"train\": {\"chosen\": TRAIN.name, \"resolved\": str(TRAIN.resolve())},\n",
    "        \"valid\": {\"chosen\": VALID.name, \"resolved\": str(VALID.resolve())},\n",
    "    },\n",
    "    \"schema\": {\"format\": \"jsonl\", \"fields\": {string_field: \"string\"}},\n",
    "}\n",
    "CONTRACT.write_text(json.dumps(contract, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# counts + hashes\n",
    "t_lines, t_bytes = count_lines_bytes(TRAIN)\n",
    "v_lines, v_bytes = count_lines_bytes(VALID)\n",
    "t_sha = sha256_file(TRAIN)\n",
    "v_sha = sha256_file(VALID)\n",
    "\n",
    "# catalog â€” write BOTH formats:\n",
    "catalog = {\n",
    "    # Newer, simpler view:\n",
    "    \"files\": {\n",
    "        \"train\": {\"path\": str(TRAIN.resolve()), \"lines\": t_lines, \"bytes\": t_bytes, \"sha256\": t_sha},\n",
    "        \"valid\": {\"path\": str(VALID.resolve()), \"lines\": v_lines, \"bytes\": v_bytes, \"sha256\": v_sha},\n",
    "    },\n",
    "    # Legacy view that Step 6 expects:\n",
    "    \"entries\": {\n",
    "        \"train\": {\n",
    "            \"path\": str(TRAIN.resolve()),\n",
    "            \"stats\": {\n",
    "                \"num_valid_examples\": t_lines,\n",
    "                \"num_bytes\": t_bytes,\n",
    "                \"sha256\": t_sha,\n",
    "            },\n",
    "        },\n",
    "        \"valid\": {\n",
    "            \"path\": str(VALID.resolve()),\n",
    "            \"stats\": {\n",
    "                \"num_valid_examples\": v_lines,\n",
    "                \"num_bytes\": v_bytes,\n",
    "                \"sha256\": v_sha,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"created_utc\": created,\n",
    "}\n",
    "\n",
    "CATALOG.write_text(json.dumps(catalog, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"=== STEP 5.5 (revised) ===\")\n",
    "print(f\"train: lines={t_lines} bytes={t_bytes:,}\")\n",
    "print(f\"valid: lines={v_lines} bytes={v_bytes:,}\")\n",
    "print(\"Wrote:\", CONTRACT.name, \"and\", CATALOG.name)\n",
    "print(\"Next: run Step 6 as-is.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1bffe0a-2bbb-4e01-beb4-47945b3d4a45",
   "metadata": {},
   "source": [
    "Step 6.\tExperiment Matrix\n",
    "\tâ€¢\tInputs: list of base models, LoRA hparams, seq len, iters/batch/accum, BF16 flag\n",
    "\tâ€¢\tOutputs: experiments.csv (one row per run), resolved iters from epochs, estimated tokens/GPU RAM budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a6c05c-12ca-46a9-8d09-65c7ffb92755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6 â€” Experiment Matrix\n",
    "# Purpose:\n",
    "#   - Define models + core hyperparams ONCE.\n",
    "#   - Resolve dataset sizes from Step 2/3 outputs.\n",
    "#   - Estimate MLX `--iters` (since mlx_lm.lora is iteration-based).\n",
    "#   - Emit a clean experiments.csv (one row per model).\n",
    "#\n",
    "# Inputs:\n",
    "#   - data_contract.json (Step 2)\n",
    "#   - data_catalog.json  (Step 2)  [preferred for counts]\n",
    "#   - data_report.json   (Step 3)  [fallback if catalog missing]\n",
    "#\n",
    "# Outputs:\n",
    "#   - experiments.csv\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, math, csv, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple, List\n",
    "\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "\n",
    "out_dir = Path(\"data\"); out_dir.mkdir(exist_ok=True)\n",
    "CONTRACT    = out_dir / cfg.paths.contract\n",
    "POLICY      = out_dir / cfg.paths.policy\n",
    "REPORT      = out_dir / cfg.paths.report\n",
    "\n",
    "RUN_DIR       = Path(cfg.run.output_dir)  # where per-model outputs will go\n",
    "EXPERIMENTS_CSV = RUN_DIR / \"experiments.csv\"\n",
    "\n",
    "# ---------- EDITABLE BLOCK ----------\n",
    "# List your MLX-compatible base models here\n",
    "BASE_MODELS = [\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    # \"mlx-community/phi-2\",\n",
    "    # add more as needed\n",
    "]\n",
    "\n",
    "# Core hyperparameters (shared across all rows; you can later copy/edit specific rows)\n",
    "EPOCHS          = 1               # convenient, weâ€™ll convert to iters\n",
    "BATCH_SIZE      = 1\n",
    "GRAD_ACCUM      = 8\n",
    "MAX_SEQ_LENGTH  = 512\n",
    "LEARNING_RATE   = 2e-4\n",
    "BF16            = True\n",
    "# Optional: override `iters` directly (0 = auto from dataset & epochs)\n",
    "ITERS_OVERRIDE  = 0\n",
    "# -----------------------------------\n",
    "\n",
    "def load_contract() -> Dict[str, Any]:\n",
    "    return json.loads(CONTRACT.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def get_counts_from_catalog() -> Tuple[int, int]:\n",
    "    if not CATALOG.exists():\n",
    "        return None, None  # signal fallback\n",
    "    c = json.loads(CATALOG.read_text(encoding=\"utf-8\"))\n",
    "    train = c[\"entries\"][\"train\"][\"stats\"][\"num_valid_examples\"]\n",
    "    # 'valid' key name may be 'valid' or 'val' depending on contract; try both\n",
    "    val_entry = c[\"entries\"].get(\"valid\") or c[\"entries\"].get(\"val\")\n",
    "    valid = val_entry[\"stats\"][\"num_valid_examples\"] if val_entry else 0\n",
    "    return int(train), int(valid)\n",
    "\n",
    "def get_counts_from_report() -> Tuple[int, int]:\n",
    "    r = json.loads(REPORT.read_text(encoding=\"utf-8\"))\n",
    "    train = r[\"splits\"][\"train\"][\"valid_examples\"]\n",
    "    val_entry = r[\"splits\"].get(\"valid\") or r[\"splits\"].get(\"val\")\n",
    "    valid = val_entry[\"valid_examples\"] if val_entry else 0\n",
    "    return int(train), int(valid)\n",
    "\n",
    "def resolve_files_from_contract(ct: Dict[str, Any]) -> Dict[str, str]:\n",
    "    files = {k: v[\"resolved\"] for k, v in ct[\"filenames\"].items() if v.get(\"resolved\")}\n",
    "    # normalize key for validation split\n",
    "    if \"valid\" in files:\n",
    "        files[\"validation\"] = files[\"valid\"]\n",
    "    elif \"val\" in files:\n",
    "        files[\"validation\"] = files[\"val\"]\n",
    "    return files\n",
    "\n",
    "def estimate_iters(num_train: int, epochs: int, batch: int, accum: int) -> int:\n",
    "    # MLX lora uses --iters; here we approximate: steps â‰ˆ epochs * num_train / (batch * accum)\n",
    "    steps = max(1, math.ceil((epochs * max(1, num_train)) / max(1, batch * accum)))\n",
    "    # also guard a reasonable floor so very tiny sets still do some learning\n",
    "    return max(100, steps)\n",
    "\n",
    "# 1) Load metadata and counts\n",
    "ct = load_contract()\n",
    "files = resolve_files_from_contract(ct)\n",
    "\n",
    "train_count, valid_count = get_counts_from_catalog()\n",
    "if train_count is None:\n",
    "    train_count, valid_count = get_counts_from_report()\n",
    "\n",
    "data_dir = Path(ct[\"data_dir\"])\n",
    "\n",
    "# 2) Build experiment rows\n",
    "rows: List[Dict[str, Any]] = []\n",
    "timestamp = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "\n",
    "for model_id in BASE_MODELS:\n",
    "    model_tag = model_id.replace(\"/\", \"--\")\n",
    "    out_root  = RUN_DIR / model_tag\n",
    "    adapter_path = out_root / \"adapter\"\n",
    "    logs_dir     = out_root / \"logs\"\n",
    "\n",
    "    iters = ITERS_OVERRIDE or estimate_iters(\n",
    "        num_train=train_count,\n",
    "        epochs=EPOCHS,\n",
    "        batch=BATCH_SIZE,\n",
    "        accum=GRAD_ACCUM,\n",
    "    )\n",
    "\n",
    "    # token budget (very rough): max_seq_length * batch * accum * iters\n",
    "    est_tokens = MAX_SEQ_LENGTH * BATCH_SIZE * GRAD_ACCUM * iters\n",
    "\n",
    "    rows.append({\n",
    "        \"created_utc\": timestamp,\n",
    "        \"model_id\": model_id,\n",
    "        \"data_dir\": str(data_dir),\n",
    "        \"train_file\": files.get(\"train\"),\n",
    "        \"valid_file\": files.get(\"validation\"),\n",
    "        \"train_examples\": train_count,\n",
    "        \"valid_examples\": valid_count,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"iters\": iters,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"grad_accum\": GRAD_ACCUM,\n",
    "        \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"bf16\": int(bool(BF16)),\n",
    "        \"adapter_path\": str(adapter_path),\n",
    "        \"log_dir\": str(logs_dir),\n",
    "        \"est_tokens\": est_tokens\n",
    "    })\n",
    "\n",
    "# 3) Write experiments.csv\n",
    "EXPERIMENTS_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "fieldnames = list(rows[0].keys()) if rows else []\n",
    "with EXPERIMENTS_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    w.writeheader()\n",
    "    for r in rows:\n",
    "        w.writerow(r)\n",
    "\n",
    "# 4) Console summary\n",
    "print(\"=== EXPERIMENT MATRIX ===\")\n",
    "print(f\"Data dir: {data_dir}\")\n",
    "print(f\"Counts: train={train_count} valid={valid_count}\")\n",
    "print(f\"Wrote: {EXPERIMENTS_CSV}\\n\")\n",
    "for r in rows:\n",
    "    print(f\"- {r['model_id']}\")\n",
    "    print(f\"   iters={r['iters']}  bs={r['batch_size']}  accum={r['grad_accum']}  max_len={r['max_seq_length']}  lr={r['learning_rate']}  bf16={r['bf16']}\")\n",
    "    print(f\"   est_tokensâ‰ˆ{r['est_tokens']:,}  adapter={r['adapter_path']}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c845e34-8c18-4585-aea9-25755d8b9c28",
   "metadata": {},
   "source": [
    "Step 7.\tLoRA Training (MLX CLI)\n",
    "\tâ€¢\tInputs: one row from experiment matrix; --data directory\n",
    "\tâ€¢\tOutputs: adapter folder; logs; scalar metrics (train_loss, val_loss by step) saved as metrics.csv and chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ea788-74ca-48ba-839e-911ca2ff9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7 (PATCH) â€” MLX LoRA command fix for 0.26.x\n",
    "# - Switch to `python -m mlx_lm lora` subcommand form\n",
    "# - Remove unsupported flags: --gradient-accumulation, --log-dir, --bf16\n",
    "# - Keep your computed `iters`, batch size, lr, max-seq-length\n",
    "# - Optional: add reporting/eval knobs that lora *does* support\n",
    "\n",
    "from __future__ import annotations\n",
    "import csv, shlex, subprocess, sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "\n",
    "out_dir = Path(\"data\"); out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "RUN_DIR       = Path(cfg.run.output_dir)  # where per-model outputs will go\n",
    "EXPERIMENTS = RUN_DIR / cfg.run.experiments\n",
    "\n",
    "\n",
    "# ---- Controls ----\n",
    "DRY_RUN = False                 # set True to just print commands\n",
    "ONLY_MODEL_ID = \"\"              # or set to a specific model_id string\n",
    "ONLY_ROW = None                 # or an integer index\n",
    "# Optional lora reporting/eval settings (set to 0 to skip passing)\n",
    "STEPS_PER_REPORT = 10\n",
    "STEPS_PER_EVAL   = 50\n",
    "VAL_BATCHES      = 1\n",
    "# ------------------\n",
    "\n",
    "def load_rows(path: Path) -> List[Dict[str, Any]]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        rows = [dict(x) for x in r]\n",
    "    for x in rows:\n",
    "        for k in (\"epochs\", \"iters\", \"batch_size\", \"grad_accum\", \"max_seq_length\", \"bf16\"):\n",
    "            if k in x and x[k] != \"\":\n",
    "                x[k] = int(float(x[k]))\n",
    "        for k in (\"learning_rate\",):\n",
    "            if k in x and x[k] != \"\":\n",
    "                x[k] = float(x[k])\n",
    "    return rows\n",
    "\n",
    "def select_rows(rows: List[Dict[str, Any]], only_model: str, only_row_idx: Optional[int]) -> List[Dict[str, Any]]:\n",
    "    if only_row_idx is not None:\n",
    "        return [rows[only_row_idx]]\n",
    "    if only_model:\n",
    "        return [r for r in rows if r.get(\"model_id\") == only_model]\n",
    "    return rows\n",
    "\n",
    "def ensure_dirs(row: Dict[str, Any]):\n",
    "    Path(row[\"adapter_path\"]).mkdir(parents=True, exist_ok=True)\n",
    "    Path(row[\"log_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_cmd(row: Dict[str, Any]) -> str:\n",
    "    py = shlex.quote(sys.executable)\n",
    "    model = shlex.quote(row[\"model_id\"])\n",
    "    data_dir = shlex.quote(row[\"data_dir\"])\n",
    "    iters = int(row[\"iters\"])\n",
    "    bs = int(row[\"batch_size\"])\n",
    "    maxlen = int(row[\"max_seq_length\"])\n",
    "    lr = float(row[\"learning_rate\"])\n",
    "    adapter = shlex.quote(row[\"adapter_path\"])\n",
    "\n",
    "    # NOTE: no --gradient-accumulation / --bf16 / --log-dir\n",
    "    parts = [\n",
    "        f\"{py} -m mlx_lm.lora\",\n",
    "        f\"--model {model}\",\n",
    "        f\"--data {data_dir}\",\n",
    "        \"--train\",\n",
    "        \"--fine-tune-type lora\",\n",
    "        f\"--batch-size {bs}\",\n",
    "        f\"--iters {iters}\",\n",
    "        f\"--learning-rate {lr}\",\n",
    "        f\"--max-seq-length {maxlen}\",\n",
    "        f\"--adapter-path {adapter}\",\n",
    "    ]\n",
    "    if VAL_BATCHES:      parts += [f\"--val-batches {int(VAL_BATCHES)}\"]\n",
    "    if STEPS_PER_REPORT: parts += [f\"--steps-per-report {int(STEPS_PER_REPORT)}\"]\n",
    "    if STEPS_PER_EVAL:   parts += [f\"--steps-per-eval {int(STEPS_PER_EVAL)}\"]\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def run_cmd(cmd: str) -> int:\n",
    "    print(\"\\n[MLX train]\", cmd)\n",
    "    if DRY_RUN:\n",
    "        print(\"DRY_RUN=True -> not executing.\")\n",
    "        return 0\n",
    "    return subprocess.run(cmd, shell=True).returncode\n",
    "\n",
    "rows = load_rows(EXPERIMENTS)\n",
    "todo = select_rows(rows, ONLY_MODEL_ID, ONLY_ROW)\n",
    "\n",
    "print(f\"Found {len(rows)} rows; running {len(todo)} row(s). DRY_RUN={DRY_RUN}\")\n",
    "for i, row in enumerate(todo):\n",
    "    print(f\"\\n=== RUN {i+1}/{len(todo)} ===\")\n",
    "    ensure_dirs(row)\n",
    "    rc = run_cmd(build_cmd(row))\n",
    "    if rc != 0:\n",
    "        print(f\"âŒ Training failed with returncode={rc}\")\n",
    "        break\n",
    "    print(\"âœ… Training launched.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d5c9f82-9cf7-4670-bc69-4da99412f65f",
   "metadata": {},
   "source": [
    "Step 8.\tArtifact Registry\n",
    "\tâ€¢\tInputs: path to base model, adapter path(s), logs\n",
    "\tâ€¢\tOutputs: artifacts.json (absolute paths, sizes, checksums), symlinks for â€œlatestâ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e93f59-0bd9-421b-bf43-76d5b4186014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8 â€” Artifact Registry\n",
    "# Reads experiments.csv and registers produced artifacts (adapters, logs).\n",
    "# - Computes SHA256 & sizes\n",
    "# - Writes artifacts.json\n",
    "# - Creates per-model symlinks: latest_adapter -> adapter , latest_logs -> logs\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, hashlib, os, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "import csv\n",
    "\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "out_dir = Path(\"data\"); out_dir.mkdir(exist_ok=True)\n",
    "RUN_DIR       = Path(cfg.run.output_dir)  # where per-model outputs will go\n",
    "EXPERIMENTS = RUN_DIR / cfg.run.experiments\n",
    "ARTIFACTS     = RUN_DIR / cfg.run.artifacts\n",
    "\n",
    "\n",
    "def sha256_file(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def gather_dir_files(root: Path) -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    if not root.exists():\n",
    "        return out\n",
    "    for p in sorted(root.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            out.append({\n",
    "                \"path\": str(p.resolve()),\n",
    "                \"rel\": str(p.relative_to(root)),\n",
    "                \"bytes\": p.stat().st_size,\n",
    "                \"sha256\": sha256_file(p),\n",
    "                \"mtime_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime(p.stat().st_mtime)),\n",
    "            })\n",
    "    return out\n",
    "\n",
    "def load_rows(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        rows = [dict(x) for x in r]\n",
    "    return rows\n",
    "\n",
    "if not EXPERIMENTS_CSV.exists():\n",
    "    raise SystemExit(\"experiments.csv not found (run Step 6).\")\n",
    "\n",
    "rows = load_rows(EXPERIMENTS)\n",
    "registry: Dict[str, Any] = {\n",
    "    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "    \"runs\": []\n",
    "}\n",
    "\n",
    "for r in rows:\n",
    "    model_id = r[\"model_id\"]\n",
    "    model_tag = model_id.replace(\"/\", \"--\")\n",
    "    out_root = Path(r[\"adapter_path\"]).parent.parent  # runs/<model_tag>\n",
    "    adapter_dir = Path(r[\"adapter_path\"])\n",
    "    logs_dir    = Path(r[\"log_dir\"])\n",
    "\n",
    "    # create handy symlinks\n",
    "    try:\n",
    "        (out_root / \"latest_adapter\").unlink(missing_ok=True)\n",
    "        (out_root / \"latest_adapter\").symlink_to(adapter_dir.name)\n",
    "    except Exception: pass\n",
    "    try:\n",
    "        (out_root / \"latest_logs\").unlink(missing_ok=True)\n",
    "        (out_root / \"latest_logs\").symlink_to(logs_dir.name)\n",
    "    except Exception: pass\n",
    "\n",
    "    entry = {\n",
    "        \"model_id\": model_id,\n",
    "        \"output_root\": str(out_root.resolve()),\n",
    "        \"adapter_dir\": str(adapter_dir.resolve()),\n",
    "        \"logs_dir\": str(logs_dir.resolve()),\n",
    "        \"files\": {\n",
    "            \"adapter\": gather_dir_files(adapter_dir),\n",
    "            \"logs\": gather_dir_files(logs_dir),\n",
    "        },\n",
    "        \"training_params\": {\n",
    "            \"iters\": int(float(r.get(\"iters\", 0) or 0)),\n",
    "            \"batch_size\": int(float(r.get(\"batch_size\", 0) or 0)),\n",
    "            \"max_seq_length\": int(float(r.get(\"max_seq_length\", 0) or 0)),\n",
    "            \"learning_rate\": float(r.get(\"learning_rate\", 0.0) or 0.0),\n",
    "        }\n",
    "    }\n",
    "    registry[\"runs\"].append(entry)\n",
    "\n",
    "ARTIFACTS.write_text(json.dumps(registry, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Console summary\n",
    "print(\"=== ARTIFACT REGISTRY ===\")\n",
    "print(\"Wrote:\", ARTIFACTS)\n",
    "for run in registry[\"runs\"]:\n",
    "    adap_files = run[\"files\"][\"adapter\"]\n",
    "    n = len(adap_files)\n",
    "    sizes = sum(f[\"bytes\"] for f in adap_files)\n",
    "    print(f\"- {run['model_id']}\")\n",
    "    print(\"   adapter_dir:\", run[\"adapter_dir\"])\n",
    "    print(\"   logs_dir:   \", run[\"logs_dir\"])\n",
    "    print(f\"   adapter files: {n}  total bytes: {sizes:,}\")\n",
    "    if n:\n",
    "        print(\"   latest:\", adap_files[-1][\"rel\"], adap_files[-1][\"bytes\"], \"bytes\")\n",
    "    print(\"   symlinks: latest_adapter, latest_logs\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "324941f9-fdce-45ef-89bf-44eeceb6d807",
   "metadata": {},
   "source": [
    "Step 9.\tFuse (optional) & Quantize (optional)\n",
    "\tâ€¢\tInputs: base model + adapter\n",
    "\tâ€¢\tOutputs: fused model path; optional MLX int4/int8 dir; update artifacts.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004d9bd-0629-4c55-a750-ab83bfdb5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9 â€” Fuse & Quantize (final, clean/idempotent)\n",
    "# - Reuses experiments.csv + artifacts.json from Steps 6â€“8\n",
    "# - If needed, fuses adapter -> fused/model  (mlx_lm fuse)\n",
    "# - Quantizes fused -> quantized/ (mlx_lm convert with explicit flags)\n",
    "# - Removes any pre-existing quantized dir to avoid MLX \"already exists\" error\n",
    "# - Updates artifacts.json\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, hashlib, time, shlex, subprocess, sys, shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "out_dir = Path(\"data\"); out_dir.mkdir(exist_ok=True)\n",
    "RUN_DIR       = Path(cfg.run.output_dir)  # where per-model outputs will go\n",
    "EXPERIMENTS = RUN_DIR / cfg.run.experiments\n",
    "ARTIFACTS     = RUN_DIR / cfg.run.artifacts\n",
    "\n",
    "# ---- Controls ----\n",
    "DO_FUSE   = True           # set False to skip fusing (if already fused)\n",
    "Q_BITS    = 4              # 4 or 8\n",
    "Q_GROUP   = 64             # e.g., 32, 64, 128\n",
    "DTYPE     = cfg.model.dtype     # float16 | bfloat16 | float32\n",
    "DRY_RUN   = False\n",
    "# -------------------\n",
    "\n",
    "def run_cmd(cmd: str) -> int:\n",
    "    print(\"[MLX]\", cmd)\n",
    "    if DRY_RUN:\n",
    "        print(\"DRY_RUN=True -> not executing.\")\n",
    "        return 0\n",
    "    return subprocess.run(cmd, shell=True).returncode\n",
    "\n",
    "def sha256_file(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def list_files(root: Path) -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    if not root.exists(): return out\n",
    "    for p in sorted(root.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            out.append({\n",
    "                \"path\": str(p.resolve()),\n",
    "                \"rel\": str(p.relative_to(root)),\n",
    "                \"bytes\": p.stat().st_size,\n",
    "                \"sha256\": sha256_file(p),\n",
    "                \"mtime_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime(p.stat().st_mtime)),\n",
    "            })\n",
    "    return out\n",
    "\n",
    "# Load artifacts (for adapter paths) and experiments (for model_ids)\n",
    "if not ARTIFACTS.exists():\n",
    "    raise SystemExit(\"artifacts.json not found. Run Steps 8/7/6 first.\")\n",
    "\n",
    "registry = json.loads(ARTIFACTS.read_text(encoding=\"utf-8\"))\n",
    "runs = registry.get(\"runs\", [])\n",
    "if not runs:\n",
    "    raise SystemExit(\"No runs found in artifacts.json.\")\n",
    "\n",
    "py = shlex.quote(sys.executable)\n",
    "updated = False\n",
    "\n",
    "for entry in runs:\n",
    "    model_id   = entry[\"model_id\"]\n",
    "    output_dir = Path(entry[\"output_root\"])\n",
    "    adapter_dir = Path(entry[\"adapter_dir\"])\n",
    "    fused_dir   = Path(entry.get(\"fused_dir\") or (output_dir / \"fused\" / \"model\"))\n",
    "\n",
    "    # 1) Fuse (optional / idempotent)\n",
    "    if DO_FUSE and not fused_dir.exists():\n",
    "        fused_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "        cmd_fuse = (\n",
    "            f\"{py} -m mlx_lm.fuse \"\n",
    "            f\"--model {shlex.quote(model_id)} \"\n",
    "            f\"--adapter-path {shlex.quote(str(adapter_dir))} \"\n",
    "            f\"--save-path {shlex.quote(str(fused_dir))}\"\n",
    "        )\n",
    "        print(\"\\n=== FUSE ===\")\n",
    "        rc = run_cmd(cmd_fuse)\n",
    "        if rc != 0:\n",
    "            print(f\"âŒ Fuse failed for {model_id}\")\n",
    "            continue\n",
    "        entry[\"fused_dir\"] = str(fused_dir.resolve())\n",
    "        entry.setdefault(\"files\", {})[\"fused\"] = list_files(fused_dir)\n",
    "        updated = True\n",
    "    elif fused_dir.exists():\n",
    "        entry[\"fused_dir\"] = str(fused_dir.resolve())\n",
    "        entry.setdefault(\"files\", {})[\"fused\"] = list_files(fused_dir)\n",
    "\n",
    "    if not fused_dir.exists():\n",
    "        print(f\"Skipping quantize for {model_id}: fused_dir missing.\")\n",
    "        continue\n",
    "\n",
    "    # 2) Quantize (idempotent + clean)\n",
    "    q_dir = output_dir / \"quantized\"\n",
    "    if q_dir.exists():\n",
    "        print(f\"Removing pre-existing quantized dir: {q_dir}\")\n",
    "        shutil.rmtree(q_dir)\n",
    "    #q_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cmd_q = (\n",
    "        f\"{py} -m mlx_lm.convert \"\n",
    "        f\"--hf-path {shlex.quote(str(fused_dir))} \"\n",
    "        f\"--mlx-path {shlex.quote(str(q_dir))} \"\n",
    "        f\"--q-bits {int(Q_BITS)} \"\n",
    "        f\"--q-group-size {int(Q_GROUP)} \"\n",
    "        f\"--dtype {shlex.quote(DTYPE)} \"\n",
    "        f\"-q\"\n",
    "    )\n",
    "    print(\"\\n=== QUANTIZE ===\")\n",
    "    rc = run_cmd(cmd_q)\n",
    "    if rc != 0:\n",
    "        print(f\"âŒ Quantize failed for {model_id}\")\n",
    "        continue\n",
    "\n",
    "    entry[\"quantized_dir\"] = str(q_dir.resolve())\n",
    "    entry[\"quantize_bits\"] = int(Q_BITS)\n",
    "    entry[\"q_group_size\"]  = int(Q_GROUP)\n",
    "    entry.setdefault(\"files\", {})[\"quantized\"] = list_files(q_dir)\n",
    "    updated = True\n",
    "\n",
    "# Save updated artifacts\n",
    "if updated:\n",
    "    registry[\"updated_utc\"] = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "    ARTIFACTS.write_text(json.dumps(registry, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== FUSE/QUANTIZE SUMMARY ===\")\n",
    "print(\"Wrote:\", ARTIFACTS)\n",
    "for entry in registry.get(\"runs\", []):\n",
    "    print(f\"- {entry['model_id']}\")\n",
    "    if \"fused_dir\" in entry:\n",
    "        print(\"   fused_dir:    \", entry['fused_dir'], f\"({len(entry.get('files',{}).get('fused',[]))} files)\")\n",
    "    if \"quantized_dir\" in entry:\n",
    "        print(\"   quantized_dir:\", entry['quantized_dir'],\n",
    "              f\"(q{entry.get('quantize_bits')}, group={entry.get('q_group_size')})\",\n",
    "              f\"files={len(entry.get('files',{}).get('quantized',[]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f8ad5-f72c-4be4-90ec-c758772013c3",
   "metadata": {},
   "source": [
    "Step 10.1 â€“ Policy-locked generation\n",
    "\tâ€¢\tNow: Uses the policy you just picked so future generations are consistent and reproducible.\n",
    "\tâ€¢\tLater: This becomes your â€œofficialâ€ inference cell. If MLX changes decode flags again, your outputs wonâ€™t change because the prompts and artifact choice are fixed.\n",
    "    \t10.\tGeneration Harness (Deterministic)\n",
    "\n",
    "\tâ€¢\tInputs: prompts, decoding params; model path (+optional adapter)\n",
    "\tâ€¢\tOutputs: raw generations with full provenance (prompt, seeds, params), generations.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49bdc94-c0c3-44f1-b3ae-f4e5766f814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 10 â€” Dynamic few-shot + anti-copy retry â†’ WRITE eval_out/generations.{jsonl,csv}\n",
    "# Adds fields: mode, generation (alias of output_text)\n",
    "from __future__ import annotations\n",
    "import os, json, random, hashlib, csv, time\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from mlx_lm import load as mlx_load, generate as mlx_generate\n",
    "\n",
    "# --- Config ---\n",
    "BASE      = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "ADAPTER   = \"runs/TinyLlama--TinyLlama-1.1B-Chat-v1.0/adapter\"\n",
    "FUSED     = \"runs/fused/model\"        # optional\n",
    "QUANT     = \"runs/quantized\"          # optional\n",
    "MAX_NEW   = 128\n",
    "SEED      = 7\n",
    "N_SHOTS   = 3\n",
    "MIN_WORDS = 3\n",
    "RETRIES   = 2\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "OUT_DIR       = Path(cfg.data.output_dir); OUT_DIR.mkdir(exist_ok=True)\n",
    "RUN_DIR       = Path(cfg.run.output_dir)  # where per-model outputs will go\n",
    "EVAL_DIR      = Path(cfg.eval.output_dir); EVAL_DIR.mkdir(exist_ok=True)\n",
    "EXPERIMENTS   = RUN_DIR / cfg.run.experiments\n",
    "ARTIFACTS     = RUN_DIR / cfg.run.artifacts\n",
    "CONTRACT      = OUT_DIR / cfg.paths.contract\n",
    "\n",
    "SEED  = cfg.run.alt_seed\n",
    "\n",
    "MODES = [\"default_eos\", \"no_eos\", \"custom_stop\"]\n",
    "CUSTOM_STOP = \"\\n\\n\"  # client-side trim for 'custom_stop'\n",
    "\n",
    "PROMPTS = [\n",
    "    \"Share a saying about time.\",\n",
    "    \"Offer a short proverb on patience.\",\n",
    "    \"Give a hopeful saying for travelers.\",\n",
    "]\n",
    "\n",
    "JSONL_PATH = EVAL_DIR / (cfg.paths.generations+\".jsonl\")\n",
    "CSV_PATH   = EVAL_DIR / (cfg.paths.generations+\".csv\")\n",
    "TOKMETA    = OUT_DIR / (cfg.paths.tokenizer+\".json\")\n",
    "\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "random.seed(SEED)\n",
    "\n",
    "# --- Load corpus for few-shot + anti-copy ---\n",
    "contract = json.loads(CONTRACT.read_text(encoding=\"utf-8\"))\n",
    "text_field = next((k for k, v in contract[\"schema\"][\"fields\"].items() if str(v).lower()==\"string\"), \"text\")\n",
    "train_path = Path(contract[\"filenames\"][\"train\"][\"resolved\"])\n",
    "\n",
    "def sha(s: str) -> str: return hashlib.sha256(s.encode(\"utf-8\",\"ignore\")).hexdigest()\n",
    "def wc(s: str) -> int:  return len(s.split())\n",
    "\n",
    "train_lines: List[str] = []\n",
    "with train_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            t = obj.get(text_field, \"\")\n",
    "            if isinstance(t, str) and t.strip():\n",
    "                train_lines.append(t.strip())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# dedupe + length buckets\n",
    "seen = set(); unique = []\n",
    "for t in train_lines:\n",
    "    h = sha(t)\n",
    "    if h not in seen:\n",
    "        seen.add(h); unique.append(t)\n",
    "short  = [t for t in unique if wc(t) <= 4]\n",
    "medium = [t for t in unique if 5 <= wc(t) <= 12]\n",
    "longer = [t for t in unique if wc(t) > 12]\n",
    "\n",
    "def pick_diverse_shots(k: int) -> List[str]:\n",
    "    pool = []\n",
    "    if short:  pool.append(random.choice(short))\n",
    "    if medium: pool.append(random.choice(medium))\n",
    "    if longer: pool.append(random.choice(longer))\n",
    "    rest = [t for t in unique if t not in pool]\n",
    "    random.shuffle(rest)\n",
    "    return (pool + rest)[:k]\n",
    "\n",
    "train_blob = \"\\n\\n\".join(unique)\n",
    "train_set  = set(unique)\n",
    "\n",
    "def format_fewshot(prompt: str, shots: List[str]) -> str:\n",
    "    return \"Some Proverbs:\\n- \" + \"\\n- \".join(shots) + f\"\\n\\n{prompt}\\n- \"\n",
    "\n",
    "def trim_on_custom_stop(text: str, stop: str) -> str:\n",
    "    i = text.find(stop)\n",
    "    return text if i == -1 else text[:i]\n",
    "\n",
    "def is_bad(gen: str) -> bool:\n",
    "    g = gen.strip()\n",
    "    if wc(g) < MIN_WORDS: return True\n",
    "    if g in train_set:    return True\n",
    "    if len(g) >= 24 and g in train_blob: return True\n",
    "    return False\n",
    "\n",
    "# choose artifact: quantized > fused > adapter\n",
    "artifact_label: str\n",
    "model_path: str\n",
    "adapter_path: Optional[str] = None\n",
    "if Path(QUANT).exists():\n",
    "    artifact_label = \"quantized\"; model_path, adapter_path = QUANT, None\n",
    "elif Path(FUSED).exists():\n",
    "    artifact_label = \"fused\";     model_path, adapter_path = FUSED, None\n",
    "else:\n",
    "    artifact_label = \"base+adapter\"; model_path, adapter_path = BASE, ADAPTER\n",
    "\n",
    "model, tok = mlx_load(model_path, adapter_path=adapter_path)\n",
    "\n",
    "# Tokenizer meta (optional but helpful)\n",
    "TOKMETA.write_text(json.dumps({\n",
    "    \"eos_token\": getattr(tok, \"eos_token\", None),\n",
    "    \"eos_token_id\": getattr(tok, \"eos_token_id\", None),\n",
    "    \"pad_token\": getattr(tok, \"pad_token\", None),\n",
    "    \"pad_token_id\": getattr(tok, \"pad_token_id\", None),\n",
    "}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def generate_once(p: str) -> tuple[str, str, list[str]]:\n",
    "    \"\"\"Return (full_prompt, generation, shots) with retry for short/copy.\"\"\"\n",
    "    tries = 0\n",
    "    while True:\n",
    "        shots = pick_diverse_shots(N_SHOTS)\n",
    "        fp = format_fewshot(p, shots)\n",
    "        txt = mlx_generate(model=model, tokenizer=tok, prompt=fp, max_tokens=MAX_NEW)\n",
    "        gen = txt[len(fp):] if txt.startswith(fp) else txt\n",
    "        gen = gen.strip()\n",
    "        if not is_bad(gen) or tries >= RETRIES:\n",
    "            return fp, gen, shots\n",
    "        tries += 1\n",
    "\n",
    "# Collect rows\n",
    "ts = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "rows = []\n",
    "\n",
    "for p in PROMPTS:\n",
    "    # default_eos\n",
    "    fp, gen, shots = generate_once(p)\n",
    "    rows.append({\n",
    "        \"timestamp\": ts, \"seed\": SEED,\n",
    "        \"model_id\": BASE, \"artifact\": artifact_label,\n",
    "        \"artifact_model_path\": model_path, \"adapter_path\": adapter_path or \"\",\n",
    "        \"prompt_variant\": \"fewshot-dynamic\", \"mode\": \"default_eos\",\n",
    "        \"prompt\": p, \"input_text\": fp,\n",
    "        \"output_text\": gen, \"generation\": gen,   # <â€” add 'generation'\n",
    "        \"shots\": shots, \"max_new_tokens\": MAX_NEW,\n",
    "    })\n",
    "    print(f\"\\n[default_eos] {p}\\nâ†’ {gen}\")\n",
    "\n",
    "    # no_eos (same call; labeled separately for grouping)\n",
    "    fp, gen, shots = generate_once(p)\n",
    "    rows.append({\n",
    "        \"timestamp\": ts, \"seed\": SEED,\n",
    "        \"model_id\": BASE, \"artifact\": artifact_label,\n",
    "        \"artifact_model_path\": model_path, \"adapter_path\": adapter_path or \"\",\n",
    "        \"prompt_variant\": \"fewshot-dynamic\", \"mode\": \"no_eos\",\n",
    "        \"prompt\": p, \"input_text\": fp,\n",
    "        \"output_text\": gen, \"generation\": gen,   # <â€” add 'generation'\n",
    "        \"shots\": shots, \"max_new_tokens\": MAX_NEW,\n",
    "    })\n",
    "    print(f\"\\n[no_eos] {p}\\nâ†’ {gen}\")\n",
    "\n",
    "    # custom_stop (client-side trim)\n",
    "    fp, gen, shots = generate_once(p)\n",
    "    gen_trim = trim_on_custom_stop(gen, CUSTOM_STOP).strip()\n",
    "    rows.append({\n",
    "        \"timestamp\": ts, \"seed\": SEED,\n",
    "        \"model_id\": BASE, \"artifact\": artifact_label,\n",
    "        \"artifact_model_path\": model_path, \"adapter_path\": adapter_path or \"\",\n",
    "        \"prompt_variant\": \"fewshot-dynamic\", \"mode\": \"custom_stop\",\n",
    "        \"prompt\": p, \"input_text\": fp,\n",
    "        \"output_text\": gen_trim, \"generation\": gen_trim,  # <â€” add 'generation'\n",
    "        \"shots\": shots, \"max_new_tokens\": MAX_NEW,\n",
    "        \"custom_stop\": CUSTOM_STOP,\n",
    "    })\n",
    "    print(f\"\\n[custom_stop] {p}\\nâ†’ {gen_trim}\")\n",
    "\n",
    "# Write JSONL\n",
    "with JSONL_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Write CSV\n",
    "csv_cols = [\"timestamp\",\"seed\",\"model_id\",\"artifact\",\"artifact_model_path\",\"adapter_path\",\n",
    "            \"prompt_variant\",\"mode\",\"prompt\",\"generation\",\"output_text\",\"shots\",\"max_new_tokens\"]\n",
    "with CSV_PATH.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=csv_cols)\n",
    "    w.writeheader()\n",
    "    for r in rows:\n",
    "        rr = r.copy(); rr[\"shots\"] = \" | \".join(r[\"shots\"])\n",
    "        w.writerow({k: rr.get(k, \"\") for k in csv_cols})\n",
    "\n",
    "print(f\"\\n=== GENERATION SUMMARY ===\")\n",
    "print(f\"Models evaluated: {BASE}\")\n",
    "print(f\"Rows: {len(rows)}  |  JSONL: {JSONL_PATH}  |  CSV: {CSV_PATH}\")\n",
    "print(f\"Modes: {MODES}\")\n",
    "print(f\"Artifacts: ['{artifact_label}']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fc843-ea21-4982-821b-7045077cb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 11 â€” EOS Behavior Probe & Quick Analysis (revised, JSONL-first)\n",
    "# Reads eval_out/generations.jsonl to avoid NaN coercion, computes the same stats,\n",
    "# and shows any rows that CSV parsing would have treated as NaN.\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, re, statistics, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "GEN_JSONL = Path(\"eval_out/generations.jsonl\")\n",
    "GEN_CSV   = Path(\"eval_out/generations.csv\")   # optional: for diagnostics only\n",
    "\n",
    "OUT_JSON  = Path(\"eval_out/eos_analysis.json\")\n",
    "OUT_SUM   = Path(\"eval_out/eos_summary.csv\")\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "OUT_DIR       = Path(cfg.data.output_dir); OUT_DIR.mkdir(exist_ok=True)\n",
    "EVAL_DIR      = Path(cfg.eval.output_dir); EVAL_DIR.mkdir(exist_ok=True)\n",
    "RUN_DIR       = Path(cfg.run.output_dir)  # where per-model outputs will go\n",
    "EXPERIMENTS   = RUN_DIR / cfg.run.experiments\n",
    "#ARTIFACTS     = RUN_DIR / cfg.run.artifacts\n",
    "#CONTRACT      = OUT_DIR / cfg.paths.contract\n",
    "GEN_JSONL     = EVAL_DIR / (cfg.paths.generations + \".jsonl\" )\n",
    "GEN_CSV       = EVAL_DIR / (cfg.paths.generations + \".csv\")\n",
    "OUT_SUM       = EVAL_DIR / (cfg.paths.summary + \".csv\")\n",
    "OUT_JSON      = EVAL_DIR / (cfg.paths.analysis + \".json\")\n",
    "\n",
    "\n",
    "if not GEN_JSONL.exists():\n",
    "    raise SystemExit(\"Missing eval_out/generations.jsonl (run Step 10).\")\n",
    "if not CONTRACT.exists():\n",
    "    raise SystemExit(\"Missing data_contract.json (from Step 2).\")\n",
    "\n",
    "# ---- Load generations from JSONL (authoritative) ----\n",
    "rows = []\n",
    "with GEN_JSONL.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            rows.append(json.loads(line))\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# ---- (Optional) CSV diagnostics: which rows would be NaN? ----\n",
    "csv_missing = pd.DataFrame()\n",
    "if GEN_CSV.exists():\n",
    "    df_csv = pd.read_csv(GEN_CSV, keep_default_na=False, na_filter=False)\n",
    "    # If your earlier CSV had blank cells, reading with default settings would coerce to NaN.\n",
    "    # keep_default_na=False prevents that. We diff the two to surface any mismatches.\n",
    "    if len(df_csv) != len(df):\n",
    "        csv_missing = pd.concat([df, df_csv]).drop_duplicates(keep=False)\n",
    "\n",
    "# ----- Helpers -----\n",
    "def word_count(s: str) -> int: return len(s.split())\n",
    "def ends_with_terminator(s: str) -> bool: return bool(re.search(r\"[.!?â€¦]$\", s.strip()))\n",
    "def has_trailing_whitespace(s: str) -> bool: return len(s) > 0 and s[-1].isspace()\n",
    "def distinct_n(tokens, n=1):\n",
    "    if len(tokens) < n: return 0.0\n",
    "    ngrams = set(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1))\n",
    "    return len(ngrams) / max(1, (len(tokens)-n+1))\n",
    "\n",
    "# Load training examples for simple memorization checks\n",
    "c = json.loads(CONTRACT.read_text(encoding=\"utf-8\"))\n",
    "train_path = Path(c[\"filenames\"][\"train\"][\"resolved\"])\n",
    "text_field = next((k for k,v in c[\"schema\"][\"fields\"].items() if str(v).lower()==\"string\"), \"text\")\n",
    "train_texts = []\n",
    "with train_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            obj = json.loads(line); t = obj.get(text_field, \"\")\n",
    "            if isinstance(t, str): train_texts.append(t.strip())\n",
    "        except Exception:\n",
    "            pass\n",
    "train_blob = \"\\n\\n\".join(train_texts)\n",
    "train_set = set(train_texts)\n",
    "\n",
    "# ----- Per-row metrics -----\n",
    "def row_metrics(r):\n",
    "    gen = str(r.get(\"generation\", \"\"))\n",
    "    toks = gen.split()\n",
    "    d1 = distinct_n(toks, 1); d2 = distinct_n(toks, 2)\n",
    "    exact_mem = gen.strip() in train_set\n",
    "    substr_mem = (not exact_mem) and (len(gen.strip()) >= 20) and (gen.strip() in train_blob)\n",
    "    return {\n",
    "        **r,\n",
    "        \"len_chars\": len(gen),\n",
    "        \"len_words\": word_count(gen),\n",
    "        \"ends_sentence\": int(ends_with_terminator(gen)),\n",
    "        \"ends_whitespace\": int(has_trailing_whitespace(gen)),\n",
    "        \"distinct1\": round(d1, 4),\n",
    "        \"distinct2\": round(d2, 4),\n",
    "        \"memorized_exact\": int(exact_mem),\n",
    "        \"memorized_substring\": int(substr_mem),\n",
    "    }\n",
    "\n",
    "m = pd.DataFrame([row_metrics(r) for r in df.to_dict(orient=\"records\")])\n",
    "\n",
    "# ----- Aggregate by mode -----\n",
    "agg = (m.groupby(\"mode\")\n",
    "         .agg(\n",
    "             n=(\"generation\",\"count\"),\n",
    "             avg_len_chars=(\"len_chars\",\"mean\"),\n",
    "             med_len_chars=(\"len_chars\",\"median\"),\n",
    "             avg_len_words=(\"len_words\",\"mean\"),\n",
    "             sent_end_rate=(\"ends_sentence\",\"mean\"),\n",
    "             trailing_ws_rate=(\"ends_whitespace\",\"mean\"),\n",
    "             distinct1_mean=(\"distinct1\",\"mean\"),\n",
    "             distinct2_mean=(\"distinct2\",\"mean\"),\n",
    "             mem_exact_rate=(\"memorized_exact\",\"mean\"),\n",
    "             mem_sub_rate=(\"memorized_substring\",\"mean\"),\n",
    "           )\n",
    "         .reset_index())\n",
    "\n",
    "for col in [\"avg_len_chars\",\"med_len_chars\",\"avg_len_words\",\"sent_end_rate\",\"trailing_ws_rate\",\"distinct1_mean\",\"distinct2_mean\",\"mem_exact_rate\",\"mem_sub_rate\"]:\n",
    "    if col in agg.columns:\n",
    "        agg[col] = agg[col].map(lambda x: round(float(x), 4))\n",
    "\n",
    "# ----- Per-prompt sample table -----\n",
    "def sample_table(df_in: pd.DataFrame, n=1):\n",
    "    out_rows = []\n",
    "    for prompt, g in df_in.groupby(\"prompt\"):\n",
    "        for mode, gg in g.groupby(\"mode\"):\n",
    "            for _, rr in gg.head(n).iterrows():\n",
    "                out_rows.append({\"prompt\": prompt, \"mode\": mode, \"generation\": rr[\"generation\"]})\n",
    "    return pd.DataFrame(out_rows)\n",
    "preview = sample_table(m, n=1)\n",
    "\n",
    "# ----- Save outputs -----\n",
    "OUT_SUM.parent.mkdir(parents=True, exist_ok=True)\n",
    "agg.to_csv(OUT_SUM, index=False)\n",
    "\n",
    "analysis = {\n",
    "    \"created_utc\": __import__(\"time\").strftime(\"%Y-%m-%dT%H:%M:%SZ\", __import__(\"time\").gmtime()),\n",
    "    \"by_mode\": agg.to_dict(orient=\"records\"),\n",
    "    \"notes\": [\n",
    "        \"JSONL is used as source of truth to avoid NaN coercion from CSV parsing.\",\n",
    "        \"distinct* ~ lexical diversity over whitespace tokens.\",\n",
    "        \"memorized_* checks generation against training set (exact / long substring).\",\n",
    "    ],\n",
    "}\n",
    "OUT_JSON.write_text(json.dumps(analysis, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ----- Console summary -----\n",
    "print(\"=== EOS / OUTPUT ANALYSIS (by mode) [JSONL] ===\")\n",
    "print(agg.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== SAMPLE OUTPUTS (1 per promptÃ—mode) ===\")\n",
    "for _, row in preview.iterrows():\n",
    "    print(f\"\\n[{row['mode']}] {row['prompt']}\\nâ†’ {row['generation']}\")\n",
    "\n",
    "if not csv_missing.empty:\n",
    "    print(\"\\n[CSV diagnostic] These rows mismatch when parsing CSV with defaults; JSONL kept them:\")\n",
    "    display(csv_missing[[\"mode\",\"prompt\",\"generation\"]].head(6))\n",
    "\n",
    "print(f\"\\nWrote: {OUT_SUM} and {OUT_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78566f65-c046-430d-9115-0d217a9465cf",
   "metadata": {},
   "source": [
    "Step 11 â€“ EOS probe & quick analysis\n",
    "\tâ€¢\tNow: Catches â€œsilentâ€ decodes, short/empty strings, trailing whitespace, sentence-end rate, and simple memorization flags. You already saw it surface empties.\n",
    "\tâ€¢\tLater: Great triage for new bases or corpora. If empties spike or distinct-n tanks, you know to adjust prompts, iters, or artifacts before deeper eval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e208b-34bb-4db4-85e6-15cc4ccae4bc",
   "metadata": {},
   "source": [
    "Step 12 â€“ Regeneration sanity checks (artifact Ã— prompt ablation)\n",
    "\tâ€¢\tNow: Proved â€œfew-shot > directive > plainâ€ and showed quantized â‰ˆ fused â‰ˆ adapter under that template.\n",
    "\tâ€¢\tLater: This is your universal â€œwhy is it quiet?â€ playbook. Swap in any model; it quickly tells you whether the failure is the artifact (quantized vs fused) or the prompting policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c373b4e1-68d3-4e31-bdb3-f2a14c5dabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 12 â€” Regeneration Sanity Checks (artifact + prompt ablation)\n",
    "# Goal: diagnose empty outputs by trying:\n",
    "#   1) artifact: quantized vs fused\n",
    "#   2) prompts: plain / directive / few-shot\n",
    "# Uses MLX defaults (no sampling kwargs) for broad compatibility.\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, textwrap, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from mlx_lm import load as mlx_load, generate as mlx_generate\n",
    "\n",
    "cfg = load_config()\n",
    "OUT_DIR       = Path(cfg.data.output_dir); OUT_DIR.mkdir(exist_ok=True)\n",
    "EVAL_DIR      = Path(cfg.eval.output_dir); EVAL_DIR.mkdir(exist_ok=True)\n",
    "RUN_DIR       = Path(cfg.run.output_dir)  # where per-model outputs will go\n",
    "EXPERIMENTS   = RUN_DIR / cfg.run.experiments\n",
    "ARTIFACTS     = RUN_DIR / cfg.run.artifacts\n",
    "CONTRACT      = OUT_DIR / cfg.paths.contract\n",
    "GEN_JSONL     = EVAL_DIR / (cfg.paths.generations + \".jsonl\" )\n",
    "GEN_CSV       = EVAL_DIR / (cfg.paths.generations + \".csv\")\n",
    "OUT_SUM       = EVAL_DIR / (cfg.paths.summary + \".csv\")\n",
    "OUT_JSON      = EVAL_DIR / (cfg.paths.analysis + \".json\")\n",
    "\n",
    "# ---- Controls ----\n",
    "ONLY_MODEL_ID = \"\"  # \"\" = all; or exact id\n",
    "PROMPTS = [\n",
    "    \"Share a saying about time.\",\n",
    "    \"Offer a short proverb on patience.\",\n",
    "    \"Give a hopeful saying for travelers.\",\n",
    "]\n",
    "MAX_NEW_TOKENS_SHORT = 64\n",
    "MAX_NEW_TOKENS_LONG  = 128\n",
    "# -------------------\n",
    "\n",
    "def load_runs() -> List[Dict[str, Any]]:\n",
    "    reg = json.loads(ARTIFACTS.read_text(encoding=\"utf-8\"))\n",
    "    runs = reg.get(\"runs\", [])\n",
    "    if ONLY_MODEL_ID:\n",
    "        runs = [r for r in runs if r.get(\"model_id\") == ONLY_MODEL_ID]\n",
    "    if not runs:\n",
    "        raise SystemExit(\"No matching runs in artifacts.json.\")\n",
    "    return runs\n",
    "\n",
    "def pick_artifacts(run_entry: Dict[str, Any]) -> List[Tuple[str, Optional[str], str]]:\n",
    "    \"\"\"Return list of (model_path, adapter_path, label) in preference order for ablation.\"\"\"\n",
    "    out = []\n",
    "    if run_entry.get(\"quantized_dir\"):\n",
    "        out.append((run_entry[\"quantized_dir\"], None, \"quantized\"))\n",
    "    if run_entry.get(\"fused_dir\"):\n",
    "        out.append((run_entry[\"fused_dir\"], None, \"fused\"))\n",
    "    # fallback: base + adapter\n",
    "    out.append((run_entry[\"model_id\"], run_entry[\"adapter_dir\"], \"base+adapter\"))\n",
    "    # dedup preserve order\n",
    "    seen = set(); uniq=[]\n",
    "    for m,a,label in out:\n",
    "        key=(m,a or \"\")\n",
    "        if key in seen: continue\n",
    "        seen.add(key); uniq.append((m,a,label))\n",
    "    return uniq\n",
    "\n",
    "# Prompt variants\n",
    "def pv_plain(prompt: str) -> str:\n",
    "    # minimal: just the instruction\n",
    "    return prompt\n",
    "\n",
    "def pv_directive(prompt: str) -> str:\n",
    "    # explicit response marker to discourage immediate EOS\n",
    "    return f\"{prompt}\\n\\nAnswer with a single important thought:\"\n",
    "\n",
    "def pv_fewshot(prompt: str) -> str:\n",
    "    # prime with a couple of aphorism-style lines, then ask\n",
    "    shots = [\n",
    "        \"The moon does not race the tide.\",\n",
    "        \"A river carves stone by lingering.\",\n",
    "    ]\n",
    "    return \"Proverbs:\\n- \" + \"\\n- \".join(shots) + f\"\\n\\n{prompt}\\n- \"\n",
    "\n",
    "PROMPT_VARIANTS = [\n",
    "    (\"plain\", pv_plain),\n",
    "    (\"directive\", pv_directive),\n",
    "    (\"fewshot\", pv_fewshot),\n",
    "]\n",
    "\n",
    "def run_generation(model_path: str, adapter_path: Optional[str], prompts: List[str], max_new: int):\n",
    "    model, tok = mlx_load(model_path, adapter_path=adapter_path or None)\n",
    "    outs=[]\n",
    "    for p in prompts:\n",
    "        txt = mlx_generate(model=model, tokenizer=tok, prompt=p, max_tokens=max_new)\n",
    "        # strip echoed prompt\n",
    "        cont = txt[len(p):] if txt.startswith(p) else txt\n",
    "        outs.append(cont.strip())\n",
    "    meta = {\n",
    "        \"eos_token\": getattr(tok, \"eos_token\", None),\n",
    "        \"eos_token_id\": getattr(tok, \"eos_token_id\", None),\n",
    "        \"pad_token\": getattr(tok, \"pad_token\", None),\n",
    "        \"pad_token_id\": getattr(tok, \"pad_token_id\", None),\n",
    "    }\n",
    "    return outs, meta\n",
    "\n",
    "def preview(text: str, width=120) -> str:\n",
    "    return textwrap.shorten(text.replace(\"\\n\",\" âŽ \"), width=width, placeholder=\"â€¦\")\n",
    "\n",
    "# Orchestrate\n",
    "runs = load_runs()\n",
    "stamp = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "rows=[]\n",
    "\n",
    "for run in runs:\n",
    "    art_list = pick_artifacts(run)\n",
    "\n",
    "    for model_path, adapter_path, art_label in art_list:\n",
    "        # Try each prompt variant, short then long budget\n",
    "        for pv_label, pv_fn in PROMPT_VARIANTS:\n",
    "            prompts_v = [pv_fn(p) for p in PROMPTS]\n",
    "\n",
    "            outs_short, meta = run_generation(model_path, adapter_path, prompts_v, MAX_NEW_TOKENS_SHORT)\n",
    "            outs_long,  _    = run_generation(model_path, adapter_path, prompts_v, MAX_NEW_TOKENS_LONG)\n",
    "\n",
    "            print(f\"\\n=== {run['model_id']} | {art_label} | {pv_label} | max_new={MAX_NEW_TOKENS_SHORT} ===\")\n",
    "            for p, o in zip(PROMPTS, outs_short):\n",
    "                print(f\"- {p}\\nâ†’ {preview(o)}\")\n",
    "\n",
    "            print(f\"\\n=== {run['model_id']} | {art_label} | {pv_label} | max_new={MAX_NEW_TOKENS_LONG} ===\")\n",
    "            for p, o in zip(PROMPTS, outs_long):\n",
    "                print(f\"- {p}\\nâ†’ {preview(o)}\")\n",
    "\n",
    "            # record minimal table\n",
    "            for budget, outs in [(\"short\", outs_short), (\"long\", outs_long)]:\n",
    "                for p, o in zip(PROMPTS, outs):\n",
    "                    rows.append({\n",
    "                        \"timestamp_utc\": stamp,\n",
    "                        \"model_id\": run[\"model_id\"],\n",
    "                        \"artifact\": art_label,\n",
    "                        \"prompt_variant\": pv_label,\n",
    "                        \"budget\": budget,\n",
    "                        \"model_path\": model_path,\n",
    "                        \"adapter_path\": adapter_path or \"\",\n",
    "                        \"eos_token\": meta[\"eos_token\"],\n",
    "                        \"eos_token_id\": meta[\"eos_token_id\"],\n",
    "                        \"prompt\": p,\n",
    "                        \"generation\": o,\n",
    "                        \"len_chars\": len(o),\n",
    "                        \"len_words\": len(o.split()),\n",
    "                        \"is_empty\": int(len(o.strip())==0),\n",
    "                    })\n",
    "\n",
    "# Save quick table\n",
    "out_path = EVAL_DIR / cfg.paths.ablations\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\nSaved detailed ablation outputs to {out_path}\")\n",
    "print(\"Tip: Look for cases where 'fused' + 'fewshot' fills in while 'quantized' + 'plain' is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e437d1-f9b9-4411-a32e-914d863676be",
   "metadata": {},
   "source": [
    "Step 13 â€“ Comparative report & policy lock-in\n",
    "\tâ€¢\tNow: Chose a winning policy (quantized + few-shot) and wrote generation_policy.json + report.md.\n",
    "\tâ€¢\tLater: This is your guardrail against drift. When you add a new model or dataset, re-run Step 12 â†’ 13 and the notebook self-tunes the generation policy instead of you hand-tuning every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627baf3-e573-4db5-a7fb-6b8f257c6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 13 â€” Comparative Report & Policy Lock-in\n",
    "# Inputs:\n",
    "#   - eval_out/ablation_generations.jsonl   (from Step 12)\n",
    "#   - artifacts.json                        (for artifact names)\n",
    "# Outputs:\n",
    "#   - eval_out/report.md\n",
    "#   - generation_policy.json  (chosen artifact & prompt template with params)\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, pandas as pd, textwrap, time\n",
    "from pathlib import Path\n",
    "\n",
    "ABL_JSONL = Path(\"eval_out/ablation_generations.jsonl\")\n",
    "ARTIFACTS = Path(\"artifacts.json\")\n",
    "REPORT_MD = Path(\"eval_out/report.md\")\n",
    "POLICY_JS = Path(\"generation_policy.json\")\n",
    "cfg = load_config()\n",
    "#OUT_DIR       = Path(cfg.data.output_dir); OUT_DIR.mkdir(exist_ok=True)\n",
    "EVAL_DIR      = Path(cfg.eval.output_dir); EVAL_DIR.mkdir(exist_ok=True)\n",
    "#EXPERIMENTS   = RUN_DIR / cfg.run.experiments\n",
    "ARTIFACTS     = RUN_DIR / cfg.run.artifacts\n",
    "#CONTRACT      = OUT_DIR / cfg.paths.contract\n",
    "#GEN_JSONL     = EVAL_DIR / (cfg.paths.generations + \".jsonl\" )\n",
    "#GEN_CSV       = EVAL_DIR / (cfg.paths.generations + \".csv\")\n",
    "OUT_SUM       = EVAL_DIR / (cfg.paths.summary + \".csv\")\n",
    "OUT_JSON      = EVAL_DIR / (cfg.paths.analysis + \".json\")\n",
    "ABL_JSONL     = EVAL_DIR / cfg.paths.ablations\n",
    "REPORT_MD     = EVAL_DIR / cfg.eval.report\n",
    "POLICY_JS     = EVAL_DIR / cfg.eval.policy\n",
    "if not ABL_JSONL.exists():\n",
    "    raise SystemExit(\"Missing eval_out/ablation_generations.jsonl (run Step 12).\")\n",
    "\n",
    "# Load\n",
    "rows = [json.loads(l) for l in ABL_JSONL.read_text(encoding=\"utf-8\").splitlines() if l.strip()]\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Score per (artifact, prompt_variant)\n",
    "def summarize(g):\n",
    "    n = len(g)\n",
    "    empty_rate = (g[\"is_empty\"].astype(int).sum()) / max(1, n)\n",
    "    sent_end_rate = (g[\"generation\"].fillna(\"\").str.strip().str.endswith(tuple(\".!?â€¦\")).astype(int).sum()) / max(1, n)\n",
    "    avg_len = g[\"len_words\"].mean()\n",
    "    med_len = g[\"len_words\"].median()\n",
    "    return pd.Series(dict(\n",
    "        n=n, empty_rate=empty_rate, sent_end_rate=sent_end_rate,\n",
    "        avg_len=round(float(avg_len),3), med_len=float(med_len)\n",
    "    ))\n",
    "\n",
    "agg = df.groupby([\"artifact\",\"prompt_variant\"], as_index=False, group_keys=False).apply(summarize)\n",
    "\n",
    "# Pick winner by heuristic:\n",
    "# 1) lowest empty_rate, 2) highest sent_end_rate, 3) highest avg_len\n",
    "winner = (agg.sort_values([\"empty_rate\",\"sent_end_rate\",\"avg_len\"], ascending=[True,False,False])\n",
    "            .iloc[0].to_dict())\n",
    "\n",
    "# Build a human-friendly table\n",
    "def pct(x): return f\"{x*100:.1f}%\"\n",
    "table = agg.copy()\n",
    "table[\"empty_rate\"] = table[\"empty_rate\"].map(pct)\n",
    "table[\"sent_end_rate\"] = table[\"sent_end_rate\"].map(pct)\n",
    "\n",
    "# Draft a short markdown report\n",
    "ts = time.strftime(\"%Y-%m-%d %H:%M:%SZ\", time.gmtime())\n",
    "lines = []\n",
    "lines += [f\"# Learning Ablation Report  \\n_{ts}_\\n\"]\n",
    "lines += [\"## Summary by artifact Ã— prompt_variant\"]\n",
    "lines += [\"\\n| artifact | prompt_variant | n | empty_rate | sent_end_rate | avg_len | med_len |\",\n",
    "          \"|---|---:|---:|---:|---:|---:|---:|\"]\n",
    "for _, r in table.iterrows():\n",
    "    lines += [f\"| {r['artifact']} | {r['prompt_variant']} | {int(r['n'])} | {r['empty_rate']} | {r['sent_end_rate']} | {r['avg_len']} | {int(r['med_len'])} |\"]\n",
    "\n",
    "lines += [\"\\n## Chosen policy\"]\n",
    "lines += [f\"- **artifact**: `{winner['artifact']}`\",\n",
    "          f\"- **prompt_variant**: `{winner['prompt_variant']}`\",\n",
    "          \"- Rationale: minimize empty outputs, then prefer clean sentence endings and adequate length.\"]\n",
    "\n",
    "# Add a tiny sample grid (first row per prompt for the winner)\n",
    "win_mask = (df[\"artifact\"]==winner[\"artifact\"]) & (df[\"prompt_variant\"]==winner[\"prompt_variant\"]) & (df[\"budget\"]==\"long\")\n",
    "sample = df[win_mask].groupby(\"prompt\").head(1)\n",
    "lines += [\"\\n## Sample outputs (winner policy)\"]\n",
    "for _, r in sample.iterrows():\n",
    "    gen = textwrap.shorten(str(r[\"generation\"]).replace(\"\\n\",\" âŽ \"), width=160, placeholder=\"â€¦\")\n",
    "    lines += [f\"- **{r['prompt']}** â†’ {gen}\"]\n",
    "\n",
    "REPORT_MD.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "print(f\"Wrote {REPORT_MD}\")\n",
    "\n",
    "# Save a reusable generation policy (Step 10 can read this later)\n",
    "# Encode few-shot template explicitly so you can tweak the shots later.\n",
    "POLICY = {\n",
    "    \"created_utc\": ts,\n",
    "    \"artifact_preference\": [winner[\"artifact\"], \"fused\", \"adapter\"],  # fallbacks\n",
    "    \"prompt_policy\": {\n",
    "        \"name\": winner[\"prompt_variant\"],\n",
    "        \"fewshot\": {\n",
    "            \"shots\": [\n",
    "                \"The moon does not race the tide.\",\n",
    "                \"A river carves stone by lingering.\"\n",
    "            ],\n",
    "            \"prefix\": \"some ideas:\\n- \",\n",
    "            \"joiner\": \"\\n- \",\n",
    "            \"suffix\": \"\\n\\n{prompt}\\n- \"\n",
    "        },\n",
    "        \"directive\": {\n",
    "            \"suffix\": \"\\n\\nAnswer with a single saying:\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "POLICY_JS.write_text(json.dumps(POLICY, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Wrote {POLICY_JS}\")\n",
    "\n",
    "# Console preview\n",
    "print(\"\\n=== WINNER ===\")\n",
    "print(f\"artifact={winner['artifact']}  prompt_variant={winner['prompt_variant']}\")\n",
    "print(\"\\n=== TABLE ===\")\n",
    "print(agg.to_string(index=False))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4886b196-4bd1-43b9-8c93-09416e952fb3",
   "metadata": {},
   "source": [
    "Step 14 â€“ Repro command builder (up next)\n",
    "\tâ€¢\tNow: Prints copy-paste shell commands (train â†’ fuse â†’ convert â†’ generate) based on your matrix and policy. Helpful for clean reruns.\n",
    "\tâ€¢\tLater: Gold for debugging regressions (â€œwhat exact command produced that?â€) and for handing runs to teammates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11367c2a-b518-471e-aba8-6524b369bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 14 â€” Repro Command Builder (exports env vars; brace-safe heredoc)\n",
    "# Writes: eval_out/repro.sh (executable), with trainâ†’fuseâ†’(optional) quantizeâ†’generate\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, csv, json, shlex, stat\n",
    "from pathlib import Path\n",
    "\n",
    "#EXPERIMENTS_CSV = Path(\"experiments.csv\")\n",
    "#OUT_DIR         = Path(\"eval_out\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "#REPRO_SH        = OUT_DIR / \"repro.sh\"\n",
    "cfg = load_config()\n",
    "OUT_DIR       = Path(cfg.data.output_dir); OUT_DIR.mkdir(exist_ok=True)\n",
    "EVAL_DIR      = Path(cfg.eval.output_dir); EVAL_DIR.mkdir(exist_ok=True)\n",
    "RUN_DIR       = Path(cfg.run.output_dir)  # where per-model outputs will go\n",
    "#EXPERIMENTS   = RUN_DIR / cfg.run.experiments\n",
    "ARTIFACTS     = RUN_DIR / cfg.run.artifacts\n",
    "\n",
    "GEN_JSONL     = EVAL_DIR / (cfg.paths.generations + \".jsonl\" )\n",
    "GEN_CSV       = EVAL_DIR / (cfg.paths.generations + \".csv\")\n",
    "OUT_SUM       = EVAL_DIR / (cfg.paths.summary + \".csv\")\n",
    "OUT_JSON      = EVAL_DIR / (cfg.paths.analysis + \".json\")\n",
    "REPRO_SH      = EVAL_DIR / cfg.eval.recreate    # a shell script to run it all again\n",
    "# ---- knobs ----\n",
    "ONLY_ROW        = 0\n",
    "DO_QUANTIZE     = True\n",
    "Q_BITS          = 4\n",
    "Q_GROUP_SIZE    = 64\n",
    "DTYPE           = \"bfloat16\"\n",
    "MAX_NEW_TOKENS  = 128\n",
    "PROMPTS         = [\n",
    "    \"Share an important saying about time.\",\n",
    "    \"Offer a short proverb on patience.\",\n",
    "    \"Give a hopeful saying for travelers.\",\n",
    "]\n",
    "# ---------------\n",
    "\n",
    "def die(msg): raise SystemExit(f\"[step14] {msg}\")\n",
    "\n",
    "# Load chosen experiment row\n",
    "if not EXPERIMENTS.exists():\n",
    "    die(\"Missing experiments.csv (run Step 5.5 then Step 6).\")\n",
    "\n",
    "rows = list(csv.DictReader(EXPERIMENTS.open(\"r\", encoding=\"utf-8\")))\n",
    "if not rows: die(\"experiments.csv has no rows.\")\n",
    "if not (0 <= ONLY_ROW < len(rows)): die(f\"ONLY_ROW {ONLY_ROW} out of range (0..{len(rows)-1}).\")\n",
    "\n",
    "row = rows[ONLY_ROW]\n",
    "need = [\"model_id\",\"data_dir\",\"iters\",\"batch_size\",\"learning_rate\",\"max_seq_length\",\"adapter_path\"]\n",
    "miss = [k for k in need if not row.get(k)]\n",
    "if miss: die(f\"Row missing fields: {miss}\")\n",
    "\n",
    "# Normalize fields\n",
    "model_id = str(row[\"model_id\"])\n",
    "data_dir = str(row[\"data_dir\"])\n",
    "iters    = int(float(row[\"iters\"]))\n",
    "bs       = int(float(row[\"batch_size\"]))\n",
    "lr       = float(row[\"learning_rate\"])\n",
    "maxlen   = int(float(row[\"max_seq_length\"]))\n",
    "adapter  = str(row[\"adapter_path\"])\n",
    "\n",
    "run_root  = str(Path(adapter).parent.parent)   # runs/<model_tag>\n",
    "fused_dir = str(Path(run_root) / \"fused\" / \"model\")\n",
    "quant_dir = str(Path(run_root) / \"quantized\")\n",
    "\n",
    "# Build the script (no f-strings inside heredoc; use env vars)\n",
    "lines = []\n",
    "def L(s=\"\"): lines.append(s)\n",
    "\n",
    "L(\"#!/usr/bin/env bash\")\n",
    "L(\"set -euo pipefail\")\n",
    "L(\"export TOKENIZERS_PARALLELISM=false\")\n",
    "L(\"\")\n",
    "# Export parameters as env vars (so heredoc Python can read them)\n",
    "L(\"# === Export experiment parameters ===\")\n",
    "L(f'export MODEL_ID={shlex.quote(model_id)}')\n",
    "L(f'export DATA_DIR={shlex.quote(data_dir)}')\n",
    "L(f'export ADAPTER={shlex.quote(adapter)}')\n",
    "L(f'export FUSED_DIR={shlex.quote(fused_dir)}')\n",
    "L(f'export QUANT_DIR={shlex.quote(quant_dir)}')\n",
    "L(f'export MAX_NEW_TOKENS={MAX_NEW_TOKENS}')\n",
    "L(f'export PROMPTS_JSON={shlex.quote(json.dumps(PROMPTS, ensure_ascii=False))}')\n",
    "L(\"\")\n",
    "\n",
    "# 1) TRAIN\n",
    "L('echo \"== 1) TRAIN (LoRA) ==\"')\n",
    "L(\"python -m mlx_lm lora \\\\\")\n",
    "L(\"  --model \\\"$MODEL_ID\\\" \\\\\")\n",
    "L(\"  --data \\\"$DATA_DIR\\\" \\\\\")\n",
    "L(\"  --train --fine-tune-type lora \\\\\")\n",
    "L(f\"  --batch-size {bs} \\\\\")\n",
    "L(f\"  --iters {iters} \\\\\")\n",
    "L(f\"  --learning-rate {lr} \\\\\")\n",
    "L(f\"  --max-seq-length {maxlen} \\\\\")\n",
    "L(\"  --adapter-path \\\"$ADAPTER\\\" \\\\\")\n",
    "L(\"  --val-batches 1 \\\\\")\n",
    "L(\"  --steps-per-report 10 \\\\\")\n",
    "L(\"  --steps-per-eval 50\")\n",
    "L(\"\")\n",
    "\n",
    "# 2) FUSE\n",
    "L('echo \"== 2) FUSE (adapter -> fused model) ==\"')\n",
    "L(\"python -m mlx_lm fuse \\\\\")\n",
    "L(\"  --model \\\"$MODEL_ID\\\" \\\\\")\n",
    "L(\"  --adapter-path \\\"$ADAPTER\\\" \\\\\")\n",
    "L(\"  --save-path \\\"$FUSED_DIR\\\"\")\n",
    "L(\"\")\n",
    "\n",
    "# 3) CONVERT (optional)\n",
    "if DO_QUANTIZE:\n",
    "    L(f'echo \"== 3) CONVERT (fused -> MLX q{Q_BITS}, group={Q_GROUP_SIZE}) ==\"')\n",
    "    L(\"rm -rf \\\"$QUANT_DIR\\\"\")\n",
    "    L(\"python -m mlx_lm convert \\\\\")\n",
    "    L(\"  --hf-path \\\"$FUSED_DIR\\\" \\\\\")\n",
    "    L(\"  --mlx-path \\\"$QUANT_DIR\\\" \\\\\")\n",
    "    L(f\"  --q-bits {Q_BITS} \\\\\")\n",
    "    L(f\"  --q-group-size {Q_GROUP_SIZE} \\\\\")\n",
    "    L(f\"  --dtype {shlex.quote(DTYPE)} \\\\\")\n",
    "    L(\"  -q\")\n",
    "else:\n",
    "    L('echo \"== 3) (SKIP) Quantization disabled ==\"')\n",
    "L(\"\")\n",
    "\n",
    "# 4) GENERATE â€” brace-safe heredoc; reads env vars inside Python\n",
    "heredoc = r\"\"\"echo \"== 4) GENERATE (policy-locked) ==\"\n",
    "python - <<'PY'\n",
    "import json, os, sys\n",
    "from pathlib import Path\n",
    "from mlx_lm import load as mlx_load, generate as mlx_generate\n",
    "\n",
    "# Read env vars\n",
    "MODEL_ID       = os.environ[\"MODEL_ID\"]\n",
    "DATA_DIR       = os.environ[\"DATA_DIR\"]\n",
    "ADAPTER        = os.environ[\"ADAPTER\"]\n",
    "FUSED_DIR      = os.environ[\"FUSED_DIR\"]\n",
    "QUANT_DIR      = os.environ[\"QUANT_DIR\"]\n",
    "MAX_NEW_TOKENS = int(os.environ[\"MAX_NEW_TOKENS\"])\n",
    "PROMPTS        = json.loads(os.environ[\"PROMPTS_JSON\"])\n",
    "\n",
    "# Policy: load if available, else fallback\n",
    "policy_path = Path(\"generation_policy.json\")\n",
    "policy = {}\n",
    "if policy_path.exists():\n",
    "    try:\n",
    "        policy = json.loads(policy_path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(\"[warn] failed to parse generation_policy.json:\", e, file=sys.stderr)\n",
    "\n",
    "if \"artifact_preference\" not in policy:\n",
    "    policy[\"artifact_preference\"] = [\"quantized\",\"fused\",\"adapter\"]\n",
    "if \"prompt_policy\" not in policy:\n",
    "    policy[\"prompt_policy\"] = {\n",
    "        \"name\": \"fewshot\",\n",
    "        \"fewshot\": {\n",
    "            \"shots\": [\n",
    "                \"The moon does not race the tide.\",\n",
    "                \"A river carves stone by lingering.\"\n",
    "            ],\n",
    "            \"prefix\": \"Proverbs:\\n- \",\n",
    "            \"joiner\": \"\\n- \",\n",
    "            \"suffix\": \"\\n\\n{prompt}\\n- \"\n",
    "        },\n",
    "        \"directive\": { \"suffix\": \"\\n\\nAnswer with a single saying:\" }\n",
    "    }\n",
    "\n",
    "# Choose artifact by policy + availability\n",
    "pref = policy.get(\"artifact_preference\", [\"quantized\",\"fused\",\"adapter\"])\n",
    "candidates = []\n",
    "if os.path.isdir(QUANT_DIR): candidates.append((\"quantized\", QUANT_DIR, None))\n",
    "if os.path.isdir(FUSED_DIR): candidates.append((\"fused\", FUSED_DIR, None))\n",
    "candidates.append((\"adapter\", MODEL_ID, ADAPTER))\n",
    "\n",
    "choice = None\n",
    "for want in pref:\n",
    "    for label, m, a in candidates:\n",
    "        if want == label:\n",
    "            choice = (label, m, a); break\n",
    "    if choice: break\n",
    "if choice is None:\n",
    "    choice = candidates[0]\n",
    "\n",
    "label, model_path, adapter_path = choice\n",
    "print(f\"[info] using artifact: {label} -> model_path={model_path} adapter={adapter_path or ''}\", file=sys.stderr)\n",
    "\n",
    "def format_prompt(pol, p):\n",
    "    name = pol[\"prompt_policy\"][\"name\"]\n",
    "    if name == \"fewshot\":\n",
    "        fp = pol[\"prompt_policy\"][\"fewshot\"]\n",
    "        return fp[\"prefix\"] + fp[\"joiner\"].join(fp[\"shots\"]) + fp[\"suffix\"].replace(\"{prompt}\", p)\n",
    "    elif name == \"directive\":\n",
    "        return p + pol[\"prompt_policy\"][\"directive\"][\"suffix\"]\n",
    "    return p\n",
    "\n",
    "model, tok = mlx_load(model_path, adapter_path=adapter_path)\n",
    "for p in PROMPTS:\n",
    "    fp  = format_prompt(policy, p)\n",
    "    out = mlx_generate(model=model, tokenizer=tok, prompt=fp, max_tokens=MAX_NEW_TOKENS)\n",
    "    gen = out[len(fp):] if out.startswith(fp) else out\n",
    "    print(f\"\\n[prompt] {p}\\nâ†’ {gen.strip()}\")\n",
    "PY\n",
    "\"\"\"\n",
    "L(heredoc)\n",
    "\n",
    "# Write file + chmod + preview\n",
    "REPRO_SH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "os.chmod(REPRO_SH, os.stat(REPRO_SH).st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)\n",
    "\n",
    "print(f\"Wrote {REPRO_SH} ({REPRO_SH.stat().st_size} bytes)\")\n",
    "print(\"Preview (head):\")\n",
    "print(\"\\n\".join(lines[:20]))\n",
    "print(\"Preview (tail):\")\n",
    "print(\"\\n\".join(lines[-20:]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "386e7215-d8a2-4827-87ee-3aebe6aee2ca",
   "metadata": {},
   "source": [
    "Step 15.\tCleanup & Freeze\n",
    "\n",
    "\tâ€¢\tInputs: artifact list\n",
    "\tâ€¢\tOutputs: pruned workspace (keep top-k checkpoints), final lockfiles, zipped handoff bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe272fd-d3bf-4b3b-8751-6f495362e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 15 â€” Cleanup & Freeze\n",
    "# - Non-destructive by default.\n",
    "# - Optionally prune bulky files (old adapter checkpoints, logs).\n",
    "# - Creates a reproducible archive with:\n",
    "#     * run_manifest.(yaml|json), requirements.lock\n",
    "#     * data_contract.json, data_catalog.json, data_report.json\n",
    "#     * experiments.csv, artifacts.json, generation_policy.json (if present)\n",
    "#     * eval_out/ (generations, reports)\n",
    "#     * runs/<model>/adapter (final) + fused/ + quantized/ (unless PRUNE_* set)\n",
    "# - Writes: dist/<project>-bundle-YYYYmmdd-HHMMSS.tar.gz + SHA256\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, time, hashlib, shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from config_loader import load_config\n",
    "cfg = load_config()\n",
    "out_dir = Path(\"data\"); out_dir.mkdir(exist_ok=True)\n",
    "RUN_DIR       = Path(cfg.run.output_dir)  # where per-model outputs will go\n",
    "EXPERIMENTS = RUN_DIR / cfg.run.experiments\n",
    "ARTIFACTS     = RUN_DIR / cfg.run.artifacts\n",
    "\n",
    "\n",
    "# ---------- Controls (edit as needed) ----------\n",
    "PROJECT_NAME = \"mlx training\"\n",
    "DIST_DIR     = Path(\"dist\")\n",
    "RUNS_DIR     = Path(\"runs\")\n",
    "EVAL_DIR     = Path(\"eval_out\")\n",
    "DATA_FILES   = [\"data_contract.json\", \"data_catalog.json\", \"data_report.json\"]\n",
    "\n",
    "# Pruning (safe defaults)\n",
    "PRUNE_OLD_ADAPTER_CHECKPOINTS = False   # True => remove numbered LoRA adapter snapshots, keep final adapters.safetensors\n",
    "PRUNE_LOGS                     = False   # True => remove runs/*/logs contents (keeps folder)\n",
    "KEEP_FUSED                     = True    # False => omit fused model from bundle\n",
    "KEEP_QUANTIZED                 = True    # False => omit quantized model from bundle\n",
    "\n",
    "# Archiving\n",
    "INCLUDE_TRAIN_DATA = False              # True => copy data/train+valid JSONL into bundle (usually keep False)\n",
    "ARCHIVE_TIMESTAMP  = time.strftime(\"%Y%m%d-%H%M%S\", time.gmtime())\n",
    "ARCHIVE_NAME       = f\"{PROJECT_NAME}-bundle-{ARCHIVE_TIMESTAMP}\"\n",
    "# ----------------------------------------------\n",
    "\n",
    "DIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STAGING = DIST_DIR / f\"{ARCHIVE_NAME}\"\n",
    "if STAGING.exists():\n",
    "    shutil.rmtree(STAGING)\n",
    "STAGING.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sha256_file(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def dir_size(p: Path) -> int:\n",
    "    total = 0\n",
    "    for root, _, files in os.walk(p):\n",
    "        for fn in files:\n",
    "            try:\n",
    "                total += (Path(root)/fn).stat().st_size\n",
    "            except Exception:\n",
    "                pass\n",
    "    return total\n",
    "\n",
    "def safe_copy(src: Path, dst: Path):\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if src.is_file():\n",
    "        shutil.copy2(src, dst)\n",
    "    elif src.is_dir():\n",
    "        if dst.exists():\n",
    "            shutil.rmtree(dst)\n",
    "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "\n",
    "# 1) Collect top-level files\n",
    "top_level = [\n",
    "    \"run_manifest.yaml\", \"run_manifest.json\", \"requirements.lock\",\n",
    "    \"experiments.csv\", \"artifacts.json\", \"generation_policy.json\"\n",
    "]\n",
    "for name in top_level + DATA_FILES:\n",
    "    p = Path(name)\n",
    "    if p.exists():\n",
    "        safe_copy(p, STAGING / p.name)\n",
    "\n",
    "# 2) Optionally include training data directory per contract\n",
    "if INCLUDE_TRAIN_DATA and Path(\"data_contract.json\").exists():\n",
    "    try:\n",
    "        c = json.loads(Path(\"data_contract.json\").read_text(encoding=\"utf-8\"))\n",
    "        data_dir = Path(c[\"data_dir\"])\n",
    "        if data_dir.exists():\n",
    "            # copy only train/valid/val jsonl\n",
    "            for fn in [\"train.jsonl\", \"valid.jsonl\", \"val.jsonl\"]:\n",
    "                q = data_dir / fn\n",
    "                if q.exists():\n",
    "                    safe_copy(q, STAGING / \"data\" / fn)\n",
    "    except Exception as e:\n",
    "        print(\"[warn] include data failed:\", e)\n",
    "\n",
    "# 3) Prepare model artifacts: runs/\n",
    "bundle_runs_root = STAGING / \"runs\"\n",
    "if RUNS_DIR.exists():\n",
    "    for model_dir in sorted(RUNS_DIR.iterdir()):\n",
    "        if not model_dir.is_dir(): continue\n",
    "        out_model_dir = bundle_runs_root / model_dir.name\n",
    "        out_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Adapter\n",
    "        adapter_dir = model_dir / \"adapter\"\n",
    "        if adapter_dir.exists():\n",
    "            # optional pruning: remove numbered checkpoints (e.g., 0000100_adapters.safetensors)\n",
    "            if PRUNE_OLD_ADAPTER_CHECKPOINTS:\n",
    "                for ck in adapter_dir.glob(\"*_adapters.safetensors\"):\n",
    "                    try: ck.unlink()\n",
    "                    except Exception: pass\n",
    "            safe_copy(adapter_dir, out_model_dir / \"adapter\")\n",
    "\n",
    "        # Logs\n",
    "        logs_dir = model_dir / \"logs\"\n",
    "        if logs_dir.exists():\n",
    "            if PRUNE_LOGS:\n",
    "                # keep folder name but clear contents\n",
    "                try:\n",
    "                    shutil.rmtree(logs_dir)\n",
    "                    logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # copy (possibly empty)\n",
    "            safe_copy(logs_dir, out_model_dir / \"logs\")\n",
    "\n",
    "        # Fused model\n",
    "        fused_dir = model_dir / \"fused\" / \"model\"\n",
    "        if fused_dir.exists() and KEEP_FUSED:\n",
    "            safe_copy(fused_dir, out_model_dir / \"fused\" / \"model\")\n",
    "\n",
    "        # Quantized model\n",
    "        quant_dir = model_dir / \"quantized\"\n",
    "        if quant_dir.exists() and KEEP_QUANTIZED:\n",
    "            safe_copy(quant_dir, out_model_dir / \"quantized\")\n",
    "\n",
    "        # Preserve convenience symlinks as real dirs/files\n",
    "        for linkname in [\"latest_adapter\", \"latest_logs\"]:\n",
    "            l = model_dir / linkname\n",
    "            if l.exists():\n",
    "                try:\n",
    "                    # resolve target and copy into staging under same name\n",
    "                    target = l.resolve()\n",
    "                    if target.is_dir():\n",
    "                        safe_copy(target, out_model_dir / linkname)\n",
    "                    elif target.is_file():\n",
    "                        safe_copy(target, out_model_dir / linkname)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "# 4) Evaluation outputs\n",
    "if EVAL_DIR.exists():\n",
    "    safe_copy(EVAL_DIR, STAGING / \"eval_out\")\n",
    "\n",
    "# 5) Size report before archiving\n",
    "staging_bytes = dir_size(STAGING)\n",
    "print(\"=== FREEZE PREVIEW ===\")\n",
    "print(f\"- Staging folder: {STAGING}  size={staging_bytes/1e6:.2f} MB\")\n",
    "print(f\"  Includes:\")\n",
    "for p in sorted(STAGING.iterdir()):\n",
    "    size_mb = dir_size(p)/1e6 if p.is_dir() else (p.stat().st_size/1e6)\n",
    "    print(f\"   â€¢ {p.name:<22} {size_mb:>7.2f} MB\")\n",
    "\n",
    "# 6) Create tar.gz\n",
    "archive_path = shutil.make_archive(str(DIST_DIR / ARCHIVE_NAME), \"gztar\", root_dir=DIST_DIR, base_dir=ARCHIVE_NAME)\n",
    "\n",
    "# 7) Hash the archive\n",
    "archive_sha = sha256_file(Path(archive_path))\n",
    "\n",
    "# 8) Write manifest for the bundle\n",
    "bundle_manifest = {\n",
    "    \"bundle_name\": Path(archive_path).name,\n",
    "    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "    \"project\": PROJECT_NAME,\n",
    "    \"paths\": {\n",
    "        \"staging\": str(STAGING.resolve()),\n",
    "        \"archive\": str(Path(archive_path).resolve())\n",
    "    },\n",
    "    \"sha256\": archive_sha,\n",
    "    \"prune\": {\n",
    "        \"old_adapter_checkpoints_removed\": bool(PRUNE_OLD_ADAPTER_CHECKPOINTS),\n",
    "        \"logs_pruned\": bool(PRUNE_LOGS),\n",
    "        \"keep_fused\": bool(KEEP_FUSED),\n",
    "        \"keep_quantized\": bool(KEEP_QUANTIZED),\n",
    "        \"include_train_data\": bool(INCLUDE_TRAIN_DATA),\n",
    "    }\n",
    "}\n",
    "(Path(archive_path).with_suffix(\".sha256\")).write_text(archive_sha + \"\\n\", encoding=\"utf-8\")\n",
    "(DIST_DIR / f\"{ARCHIVE_NAME}.manifest.json\").write_text(json.dumps(bundle_manifest, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n=== FREEZE COMPLETE ===\")\n",
    "print(f\"Bundle: {archive_path}\")\n",
    "print(f\"SHA256: {archive_sha}\")\n",
    "print(f\"Manifest: {DIST_DIR / (ARCHIVE_NAME + '.manifest.json')}\")\n",
    "print(\"Tip: verify with  shasum -a 256 <archive>  and compare.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a2e7d1-2385-43d0-a2d8-52cb0cfb436f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
