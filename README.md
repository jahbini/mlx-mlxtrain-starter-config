> âš¡ï¸ Built with the help of OpenAIâ€™s ChatGPT, iteratively co-developed into a working training pipeline.
> ### Explanation and Apology
> I am NOT an expert in AI, or Python, or Notebooks, However I found I needed to train my M4 Mac with some journals I had written.
>
> I was amazed at the confusing, conflicting and version specific suggestions on Youtube and the web.  And I was desperate, so I
asked GPT-5 to make me a notebook.  We built this notebook over three days and it seems to do what every AI dreamer dreams about.
>
> Have fun, or a similar emotion.
>
> Geemo Beamo
# MLX Fine-Tuning Starter (Apple Silicon)

End-to-end LoRA fine-tuning on Apple hardware using **MLX-LM** â€” no CUDA.
Notebook includes: data prep â†’ training â†’ fuse/quantize â†’ eval/ablation â†’ repro script â†’ freeze bundle.

## Requirements
- Apple Silicon (M-series)
- macOS 14+
- Conda or Python 3.11
  Install Conda from https://www.anaconda.com/download

## Setup
```bash
conda env create -f environment.yml -n mlxtrain
conda activate mlxtrain
jupyter lab

## What youâ€™ll commit vs. generate

**Committed at Github:**
- `README.md`, `LICENSE`, `.gitignore`
- `environment.yml` (or `requirements.txt`)
- `notebooks/mlx_finetune.ipynb`

**Generated by the notebook as it runs and ignored by git:**
- `data/train.jsonl`, `data/valid.jsonl`
- `experiments.csv`, `artifacts.json`, `generation_policy.json`
- `runs/â€¦` (adapters/fused/quantized)
- `eval_out/â€¦` (reports, final_generations, `repro.sh`)
- `dist/*.tar.gz` (freeze bundles)

## ğŸ“‚ Notebook Outputs

Running the training notebook produces several files and directories.  
Hereâ€™s what they are for:

- **`data/train.jsonl`**, **`data/valid.jsonl`**  
  Processed training and validation splits in JSONL format.  

- **`data_contract.json`**  
  Describes dataset schema (field names, types) and file locations.  

- **`data_catalog.json`**  
  Records dataset statistics (counts, sizes) for train/valid splits.  

- **`data_report.json`**  
  Extended dataset report with quality checks and summary info.  

- **`experiments.csv`**  
  Log of training runs, hyperparameters, and adapter paths.  

- **`generation_policy.json`**  
  Rules for prompt formatting and artifact preference during generation.  

- **`artifacts.json`**  
  Index of produced artifacts (adapter, fused, quantized models).  

- **`run_manifest.yaml`**  
  Summary manifest of a training run: configs, inputs, and outputs.  

- **`requirements.lock`**  
  Frozen Python dependencies used during training.  

- **`runs/`**  
  Directory holding actual model outputs:  
  - `adapter/` â€“ LoRA adapter weights  
  - `fused/` â€“ full fused model  
  - `quantized/` â€“ quantized version for efficient inference  

- **`eval_out/`**  
  Evaluation outputs, including:  
  - `report.md` â€“ eval summary  
  - `repro.sh` â€“ reproducible bash script to re-run the run  

- **`dist/`**  
  â€œFrozenâ€ bundles for distribution (tarball archives with manifest + run files).

## ğŸš€ Minimal Essential Files

If you just want to train and use your model quickly, the key outputs are:

- **`data/train.jsonl`**, **`data/valid.jsonl`**  
  Training and validation data.

- **`runs/adapter/`**  
  LoRA adapter weights (can be loaded into the base model).

- **`runs/fused/`**  
  Full fused model (adapter + base merged).

- **`runs/quantized/`**  
  Quantized model for efficient inference on Apple Silicon.

- **`eval_out/repro.sh`**  
  Bash script to reproduce training, fusion, quantization, and generation. 

---
