> ⚡️ Built with the help of OpenAI’s ChatGPT, iteratively co-developed into a working training pipeline.
> ### Explanation and Apology
> I am NOT an expert in AI, or Python, or Notebooks, However I found I needed to train my M4 Mac with some journals I had written.
>
> I was amazed at the confusing, conflicting and version specific suggestions on Youtube and the web.  And I was desperate, so I
asked GPT-5 to make me a notebook.  We built this notebook over three days and it seems to do what every AI dreamer dreams about.
>
> Have fun, or a similar emotion.
>
> Geemo Beamo
> PS: after I regained my sanity, I thought that a straight forward makefile would be better able to run many tests against any dataset.
> And so we have
# MLX Fine-Tuning Pipeline (Apple Silicon)

End-to-end LoRA fine-tuning on Apple hardware using **MLX-LM** — no CUDA.
Pipeline includes: data prep → training → fuse/quantize → eval/ablation → repro script → freeze bundle.

## Requirements
- Apple Silicon (M-series)
- macOS 14+
- Conda or Python 3.11
  Install Conda from https://www.anaconda.com/download

## Setup -- OLD
```bash
conda env create -f environment.yml -n mlxtrain
conda activate mlxtrain
jupyter lab
```

## Setup -- NEW
Edit the override.yaml with your models (config.experiments) and dataSet, and then
```bash
make all
```

This starter has been re-factored into a Makefile oriented
pipeline of scripts to take configuration stuff from a config/default.yaml (with a local override.yaml) to specify parameters and file paths.  The notebook is there only for referance purposes.

This first pipeline allows any compatible HuggingFace dataset to be trained on several models via experiments in identical fashion.  You are able to specify
all these in the local override.yaml.

**Generated by the pipeline as it runs and ignored by git:**
- `data/train.jsonl`, `data/valid.jsonl`
- `experiments.csv`, `artifacts.json`, `generation_policy.json`
- `runs/…` (adapters/fused/quantized)
- `eval_out/…` (reports, final_generations, `repro.sh`)
- `dist/*.tar.gz` (freeze bundles)

## Outputs

Running the training pipeline produces several files and directories.  
Here’s what they are for:

- **`data/train.jsonl`**, **`data/valid.jsonl`**  
  Processed training and validation splits in JSONL format.  

- **`data_contract.json`**  
  Describes dataset schema (field names, types) and file locations.  

- **`data_catalog.json`**  
  Records dataset statistics (counts, sizes) for train/valid splits.  

- **`data_report.json`**  
  Extended dataset report with quality checks and summary info.  

- **`experiments.csv`**  
  Log of training runs, hyperparameters, and adapter paths.  

- **`generation_policy.json`**  
  Rules for prompt formatting and artifact preference during generation.  

- **`artifacts.json`**  
  Index of produced artifacts (adapter, fused, quantized models).  

- **`run_manifest.yaml`**  
  Summary manifest of a training run: configs, inputs, and outputs.  

- **`requirements.lock`**  
  Frozen Python dependencies used during training.  

- **`runs/`**  
  Directory holding actual model outputs:  
  - `adapter/` – LoRA adapter weights  
  - `fused/` – full fused model  
  - `quantized/` – quantized version for efficient inference  

- **`eval_out/`**  
  Evaluation outputs, including:  
  - `report.md` – eval summary  
  - `repro.sh` – reproducible bash script to re-run the run  

- **`dist/`**  
  “Frozen” bundles for distribution (tarball archives with manifest + run files).
