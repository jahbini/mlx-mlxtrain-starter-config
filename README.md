> ‚ö°Ô∏è Built with the help of OpenAI‚Äôs ChatGPT, iteratively co-developed into a working training pipeline.
> ### Explanation and Apology
> I am NOT an expert in AI, or Python, or Notebooks, However I found I needed to train my M4 Mac with some journals I had written.
>
> I was amazed at the confusing, conflicting and version specific suggestions on Youtube and the web.  And I was desperate, so I
asked GPT-5 to make me a notebook.  We built this notebook over three days and it seems to do what every AI dreamer dreams about.
>
> Have fun, or a similar emotion.
>
> Geemo Beamo
# MLX Fine-Tuning Starter (Apple Silicon)

End-to-end LoRA fine-tuning on Apple hardware using **MLX-LM** ‚Äî no CUDA.
Notebook includes: data prep ‚Üí training ‚Üí fuse/quantize ‚Üí eval/ablation ‚Üí repro script ‚Üí freeze bundle.

## Requirements
- Apple Silicon (M-series)
- macOS 14+
- Conda or Python 3.11
  Install Conda from https://www.anaconda.com/download

## Setup
```bash
conda env create -f environment.yml -n mlxtrain
conda activate mlxtrain
jupyter lab

## What you‚Äôll commit vs. generate

**Committed at Github:**
- `README.md`, `LICENSE`, `.gitignore`
- `environment.yml` (or `requirements.txt`)
- `notebooks/mlx_finetune.ipynb`

**Generated by the notebook as it runs and ignored by git:**
- `data/train.jsonl`, `data/valid.jsonl`
- `experiments.csv`, `artifacts.json`, `generation_policy.json`
- `runs/‚Ä¶` (adapters/fused/quantized)
- `eval_out/‚Ä¶` (reports, final_generations, `repro.sh`)
- `dist/*.tar.gz` (freeze bundles)

## üìÇ Notebook Outputs

Running the training notebook produces several files and directories.  
Here‚Äôs what they are for:

- **`data/train.jsonl`**, **`data/valid.jsonl`**  
  Processed training and validation splits in JSONL format.  

- **`data_contract.json`**  
  Describes dataset schema (field names, types) and file locations.  

- **`data_catalog.json`**  
  Records dataset statistics (counts, sizes) for train/valid splits.  

- **`data_report.json`**  
  Extended dataset report with quality checks and summary info.  

- **`experiments.csv`**  
  Log of training runs, hyperparameters, and adapter paths.  

- **`generation_policy.json`**  
  Rules for prompt formatting and artifact preference during generation.  

- **`artifacts.json`**  
  Index of produced artifacts (adapter, fused, quantized models).  

- **`run_manifest.yaml`**  
  Summary manifest of a training run: configs, inputs, and outputs.  

- **`requirements.lock`**  
  Frozen Python dependencies used during training.  

- **`runs/`**  
  Directory holding actual model outputs:  
  - `adapter/` ‚Äì LoRA adapter weights  
  - `fused/` ‚Äì full fused model  
  - `quantized/` ‚Äì quantized version for efficient inference  

- **`eval_out/`**  
  Evaluation outputs, including:  
  - `report.md` ‚Äì eval summary  
  - `repro.sh` ‚Äì reproducible bash script to re-run the run  

- **`dist/`**  
  ‚ÄúFrozen‚Äù bundles for distribution (tarball archives with manifest + run files).

## üöÄ Minimal Essential Files

If you just want to train and use your model quickly, the key outputs are:

- **`data/train.jsonl`**, **`data/valid.jsonl`**  
  Training and validation data.

- **`runs/adapter/`**  
  LoRA adapter weights (can be loaded into the base model).

- **`runs/fused/`**  
  Full fused model (adapter + base merged).

- **`runs/quantized/`**  
  Quantized model for efficient inference on Apple Silicon.

- **`eval_out/repro.sh`**  
  Bash script to reproduce training, fusion, quantization, and generation. 

---

### layout

repo/
  configs/
    default.yaml
  scripts/
    00_setup_data_dir.py         # (web scrape / layout init)
    01_fetch_hf_dataset.py
    02_prepare_data.py
    03_train.py
    04_eval.py
    05_report.py
  mlxtrain/
    __init__.py
    utils.py                     # load_config(), save_json(), set_logger(), etc.
  notebooks/
    walkthrough.ipynb            # calls scripts and shows outputs
  Makefile                       # optional: `make train`, `make all`
  README.md
